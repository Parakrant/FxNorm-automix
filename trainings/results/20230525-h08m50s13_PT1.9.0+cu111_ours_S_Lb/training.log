Starting training:
	        Results folder: ../trainings/results/20230525-h08m50s13_PT1.9.0+cu111_ours_S_Lb
	    Configuration file: ../configs/ISMIR/ours_S_Lb.py
	Experiment description: ours_S_Lb


Global variables:
{   'AugmentationChain': <class 'automix.common_audioeffects.AugmentationChain'>,
    'DataType': <enum 'DataType'>,
    'Loss': <class 'automix.common_losses.Loss'>,
    'Net': <class 'automix.common_networkbuilding_cafx_tdcn_lstm_mix.Net'>,
    'OrderedDict': <class 'collections.OrderedDict'>,
    'Pool': <bound method BaseContext.Pool of <multiprocessing.context.DefaultContext object at 0x7f5cf11f0d50>>,
    'Process': <class 'multiprocessing.context.Process'>,
    'RF': 505,
    'RawArray': <bound method BaseContext.RawArray of <multiprocessing.context.DefaultContext object at 0x7f5cf11f0d50>>,
    'SimpleQueue': <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f5cf11f0d50>>,
    'StereoLoss': <class 'automix.common_losses.StereoLoss'>,
    'StereoLoss2': <class 'automix.common_losses.StereoLoss2'>,
    'SummaryWriter': <class 'torch.utils.tensorboard.writer.SummaryWriter'>,
    'SuperNet': <class 'automix.common_supernet.SuperNet'>,
    'ThreadPool': <class 'multiprocessing.pool.ThreadPool'>,
    '__annotations__': {},
    '__builtins__': <module 'builtins' (built-in)>,
    '__cached__': None,
    '__doc__': 'Config file.',
    '__file__': '../automix/train.py',
    '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f5cf8b2fed0>,
    '__name__': '__main__',
    '__package__': None,
    '__spec__': None,
    '__warningregistry__': {'version': 81},
    'amp': <module 'torch.cuda.amp' from '/home/venv/lib/python3.7/site-packages/torch/cuda/amp/__init__.py'>,
    'argparse': <module 'argparse' from '/home/venv/lib/python3.7/argparse.py'>,
    'args': Namespace(config_file='../configs/ISMIR/ours_S_Lb.py', description='ours_S_Lb', folder_suffix='ours_S_Lb', results_folder='../trainings', weight_initialization=['../trainings/results/ours_S_pretrained/current_model_for_mixture.params']),
    'compute_receptive_field': <function compute_receptive_field at 0x7f5be9f21440>,
    'compute_stft': <function compute_stft at 0x7f5bea1e9c20>,
    'config': {   'ACCEPTED_SAMPLING_RATES': [44100],
                  'AMSGRAD': True,
                  'AUGMENTER_CHAIN': AugmentationChain(fxs=[], shuffle=False),
                  'AUGMENTER_PADDING': (0, 0),
                  'AUGMENTER_SOURCES': [],
                  'BATCHED_TEST': False,
                  'BATCHED_VALID': True,
                  'BATCH_SIZE': 4,
                  'CALCULATE_STATISTICS': True,
                  'CUDNN_BENCHMARK': True,
                  'DATA_DIR_TRAIN': [('/home/FxNorm-automix/train', False)],
                  'DATA_DIR_VALID': [('/home/FxNorm-automix/val', False)],
                  'DEBUG': False,
                  'FFT_SIZE': 4096,
                  'GRAD_CLIP_MAX_NORM': 0.2,
                  'GRAD_CLIP_NORM_TYPE': 2,
                  'GUARD_LEFT': 32446,
                  'GUARD_RIGHT': 32446,
                  'HOP_LENGTH': 1024,
                  'INITIAL_LEARNING_RATE': 0.001,
                  'INIT_NETWORK': None,
                  'INPUTS': ['vocals_normalized', 'bass_normalized', 'drums_normalized', 'other_normalized'],
                  'KERNEL_SIZE_ENCODER': 64,
                  'KERNEL_SIZE_TB': 3,
                  'L2_REGULARIZATION': 1e-07,
                  'LEARNING_RATES': [   (1, 0.0),
                                        (150, 0.001),
                                        (50, 0.0003333333333333333),
                                        (50, 0.0001),
                                        (25, 3.3333333333333335e-05),
                                        (25, 1e-05),
                                        (10, 1e-06)],
                  'MAPPED_SOURCES': {},
                  'MAX_POOLING': 64,
                  'MAX_PROCESSED_FREQUENCY': 16000.0,
                  'MAX_VALIDATION_SEQ_LENGTH_TD': 10584000,
                  'NET_TYPE': 'CAFX_TDCN_MIX',
                  'NUM_DATAPROVIDING_PROCESSES': 8,
                  'NUM_EPOCHS': 311,
                  'NUM_MINIBATCHES_FOR_STATISTICS_ESTIMATION': 1000,
                  'NUM_MINIBATCHES_PER_EPOCH': 100,
                  'N_BINS': 2049,
                  'N_BINS_KEEP': 1486,
                  'N_CHANNELS': 2,
                  'N_FEATURES_ENCODER': 128,
                  'N_FEATURES_OUT': 64,
                  'N_FEATURES_SEPARATION_MODULE': 256,
                  'N_FEATURES_TB': 128,
                  'N_REPEATS': 4,
                  'N_TB_PER_REPEAT': 6,
                  'OUTPUTS': ['mixture'],
                  'OVERLAP_PROBABILITY': {},
                  'PRESENT_PROBABILITY': {},
                  'PRETRAIN': False,
                  'PRETRAIN_FRONT_END': True,
                  'QUANTIZATION_BW': None,
                  'QUANTIZATION_OP': None,
                  'SAVE_NET_AT_EPOCHS': [],
                  'SE_AMP_RATIO': 16,
                  'SHUFFLE_CHANNELS': True,
                  'SHUFFLE_STEMS': False,
                  'SOURCES': [   'vocals_normalized',
                                 'bass_normalized',
                                 'drums_normalized',
                                 'other_normalized',
                                 'mixture'],
                  'STFT_WINDOW': array([0.        , 0.00076699, 0.00153398, ..., 0.00230097, 0.00153398,
       0.00076699]),
                  'TARGETS': [('mixture',)],
                  'TENSORBOARD': True,
                  'TRAINING_SEQ_LENGTH': 440960,
                  'TRAIN_LOSSES': [   StereoLoss2(
  (_loss): Loss(
    (_loss): L1Loss()
  )
  (fir): FIRFilterLoss(
    (_loss): Loss(
      (_loss): L1Loss()
    )
  )
  (lossMSE): MSELoss()
)],
                  'USE_AMP': True,
                  'VALID_LOSSES': OrderedDict([   (   'stereo_loss',
                                                      StereoLoss2(
  (_loss): Loss(
    (_loss): L1Loss()
  )
  (fir): FIRFilterLoss(
    (_loss): Loss(
      (_loss): L1Loss()
    )
  )
  (lossMSE): MSELoss()
))])},
    'copyfile': <function copyfile at 0x7f5cf8713290>,
    'cpu_count': <bound method BaseContext.cpu_count of <multiprocessing.context.DefaultContext object at 0x7f5cf11f0d50>>,
    'create_dataset_mixing': <function create_dataset_mixing at 0x7f5bea1f6200>,
    'create_minibatch_mixing': <function create_minibatch_mixing at 0x7f5bea1f6320>,
    'f': <_io.TextIOWrapper name='../trainings/results/20230525-h08m50s13_PT1.9.0+cu111_ours_S_Lb/settings.log' mode='w' encoding='UTF-8'>,
    'f_guard': 30,
    'get_process_memory': <function get_process_memory at 0x7f5bea1e9dd0>,
    'guard': 32446,
    'loss': StereoLoss2(
  (_loss): Loss(
    (_loss): L1Loss()
  )
  (fir): FIRFilterLoss(
    (_loss): Loss(
      (_loss): L1Loss()
    )
  )
  (lossMSE): MSELoss()
),
    'ngpus': 1,
    'ngpus_per_src': 1,
    'nn': <module 'torch.nn' from '/home/venv/lib/python3.7/site-packages/torch/nn/__init__.py'>,
    'np': <module 'numpy' from '/home/venv/lib/python3.7/site-packages/numpy/__init__.py'>,
    'optim': <module 'torch.optim' from '/home/venv/lib/python3.7/site-packages/torch/optim/__init__.py'>,
    'os': <module 'os' from '/home/venv/lib/python3.7/os.py'>,
    'parser': ArgumentParser(prog='train.py', usage=None, description='Training parser', formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True),
    'pformat': <function pformat at 0x7f5cf876c440>,
    'plt': <module 'matplotlib.pyplot' from '/home/venv/lib/python3.7/site-packages/matplotlib/pyplot.py'>,
    'random': <module 'random' from '/home/venv/lib/python3.7/random.py'>,
    'recursive_getattr': <function recursive_getattr at 0x7f5bea1e4b00>,
    'results_folder': '../trainings/results/20230525-h08m50s13_PT1.9.0+cu111_ours_S_Lb',
    'seq_length_td': 440960,
    'sklearn': <module 'sklearn' from '/home/venv/lib/python3.7/site-packages/sklearn/__init__.py'>,
    'socket': <module 'socket' from '/home/venv/lib/python3.7/socket.py'>,
    'subprocess': <module 'subprocess' from '/home/venv/lib/python3.7/subprocess.py'>,
    'sys': <module 'sys' (built-in)>,
    'td_length_from_fd': <function td_length_from_fd at 0x7f5bea1f6050>,
    'tee': <subprocess.Popen object at 0x7f5bea20eed0>,
    'time': <module 'time' (built-in)>,
    'torch': <module 'torch' from '/home/venv/lib/python3.7/site-packages/torch/__init__.py'>,
    'uprint': <function uprint at 0x7f5bf1ad5a70>}


Environment variables:
{   'CONDA_DEFAULT_ENV': '/home/venv',
    'CONDA_EXE': '/home/enter/bin/conda',
    'CONDA_PREFIX': '/home/venv',
    'CONDA_PREFIX_1': '/home/enter',
    'CONDA_PROMPT_MODIFIER': '(/home/venv) ',
    'CONDA_PYTHON_EXE': '/home/enter/bin/python',
    'CONDA_SHLVL': '2',
    'CONFIGS_FOLDER': '../configs/ISMIR',
    'CUDA_VISIBLE_DEVICES': '0',
    'DESCRIPTION': 'ours_S_Lb',
    'FOLDER_SUFFIX': 'ours_S_Lb',
    'HOME': '/root',
    'KMP_DUPLICATE_LIB_OK': 'True',
    'KMP_INIT_AT_FORK': 'FALSE',
    'LC_CTYPE': 'C.UTF-8',
    'LESSCLOSE': '/usr/bin/lesspipe %s %s',
    'LESSOPEN': '| /usr/bin/lesspipe %s',
    'LOGNAME': 'root',
    'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:',
    'MOTD_SHOWN': 'pam',
    'OLDPWD': '/home/FxNorm-automix',
    'OMP_NUM_THREADS': '1',
    'PATH': '/home/venv/bin:/home/enter/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',
    'PWD': '/home/FxNorm-automix/scripts',
    'RESULTS_FOLDER': '../trainings',
    'SHELL': '/bin/bash',
    'SHLVL': '2',
    'SSH_CLIENT': '203.198.8.93 62592 22',
    'SSH_CONNECTION': '203.198.8.93 62592 172.17.0.2 22',
    'SSH_TTY': '/dev/pts/0',
    'TERM': 'screen',
    'TMUX': '/tmp/tmux-0/default,482,0',
    'TMUX_PANE': '%0',
    'USER': 'root',
    '_': '/home/venv/bin/python',
    '_CE_CONDA': '',
    '_CE_M': ''}

Maximum validation length is 4.00m
Create dataset (train) ...

Creating dataset for path=/home/FxNorm-automix/train ...
Processing mixture (1 of 1): song1
	Ignoring unknown source from file bass.wav
	Adding function handle for "bass_normalized" from file bass_normalized.wav
	Ignoring unknown source from file drums.wav
	Adding function handle for "drums_normalized" from file drums_normalized.wav
	Adding function handle for "mixture" from file mixture.wav
	Ignoring unknown source from file other.wav
	Adding function handle for "other_normalized" from file other_normalized.wav
	Ignoring unknown source from file vocals.wav
	Adding function handle for "vocals_normalized" from file vocals_normalized.wav
Finished preparation of dataset. Found in total the following material (in 1 directories):
	bass_normalized: 0.06 hours
	drums_normalized: 0.06 hours
	mixture: 0.06 hours
	other_normalized: 0.06 hours
	vocals_normalized: 0.06 hours
	took 0.00s, current memory consumption is 1.13GB

Create dataset (valid) ...

Creating dataset for path=/home/FxNorm-automix/val ...
Processing mixture (1 of 1): song1
	Ignoring unknown source from file bass.wav
	Adding function handle for "bass_normalized" from file bass_normalized.wav
	Ignoring unknown source from file drums.wav
	Adding function handle for "drums_normalized" from file drums_normalized.wav
	Adding function handle for "mixture" from file mixture.wav
	Ignoring unknown source from file other.wav
	Adding function handle for "other_normalized" from file other_normalized.wav
	Ignoring unknown source from file vocals.wav
	Adding function handle for "vocals_normalized" from file vocals_normalized.wav
Finished preparation of dataset. Found in total the following material (in 1 directories):
	bass_normalized: 0.06 hours
	drums_normalized: 0.06 hours
	mixture: 0.06 hours
	other_normalized: 0.06 hours
	vocals_normalized: 0.06 hours
	took 0.00s, current memory consumption is 1.13GB

Compute baseline losses ...
	Lower baselines on validation dataset (time-domain, l1):
		Mean	Median
	mixture	0.048	0.048

	Lower baselines on validation dataset (freq-domain, l1):
		Mean	Median
	mixture	0.732	0.732

	Lower baselines on validation dataset (time-domain, l2):
		Mean	Median
	mixture	0.004	0.004

	Lower baselines on validation dataset (freq-domain, l2):
		Mean	Median
	mixture	6.643	6.643

	took 3.72s, current memory consumption is 1.33GB

Compute mean/scale on training set ...
	took 0.55s, current memory consumption is 1.33GB

Create shared memory variables ...
	took 0.24s, current memory consumption is 1.85GB

Starting data providing process 0 with random seed 2011284583 and 201137315
Starting data providing process 1 with random seed 3302469462 and 1283724373
Starting data providing process 2 with random seed 1198028621 and 4113263280
Starting data providing process 3 with random seed 1100854367 and 814733831
Starting data providing process 4 with random seed 2790413483 and 2247445274
Starting data providing process 5 with random seed 4064045639 and 1566827443
Starting data providing process 6 with random seed 3011041632 and 3211227313
Starting data providing process 7 with random seed 1287002960 and 3195404937
WARNING: Could not save TorchScript model.

Network parameters for CAFX_TDCN_MIX:
	layerNorm.weight              	[128]                    	128
	layerNorm.bias                	[128]                    	128
	bottleneck_conv1x1.weight     	[256, 128, 1]            	32768
	bottleneck_conv1x1.bias       	[256]                    	256
	repeats.0.0.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.0.0.conv1x1.bias      	[128]                    	128
	repeats.0.0.nonlinearity1.weight	[1]                      	1
	repeats.0.0.norm1.weight      	[128]                    	128
	repeats.0.0.norm1.bias        	[128]                    	128
	repeats.0.0.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.0.0.depthwise_conv.bias	[128]                    	128
	repeats.0.0.nonlinearity2.weight	[1]                      	1
	repeats.0.0.norm2.weight      	[128]                    	128
	repeats.0.0.norm2.bias        	[128]                    	128
	repeats.0.0.skip_out.weight   	[64, 128, 1]             	8192
	repeats.0.0.skip_out.bias     	[64]                     	64
	repeats.0.0.res_out.weight    	[256, 128, 1]            	32768
	repeats.0.0.res_out.bias      	[256]                    	256
	repeats.0.1.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.0.1.conv1x1.bias      	[128]                    	128
	repeats.0.1.nonlinearity1.weight	[1]                      	1
	repeats.0.1.norm1.weight      	[128]                    	128
	repeats.0.1.norm1.bias        	[128]                    	128
	repeats.0.1.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.0.1.depthwise_conv.bias	[128]                    	128
	repeats.0.1.nonlinearity2.weight	[1]                      	1
	repeats.0.1.norm2.weight      	[128]                    	128
	repeats.0.1.norm2.bias        	[128]                    	128
	repeats.0.1.skip_out.weight   	[64, 128, 1]             	8192
	repeats.0.1.skip_out.bias     	[64]                     	64
	repeats.0.1.res_out.weight    	[256, 128, 1]            	32768
	repeats.0.1.res_out.bias      	[256]                    	256
	repeats.0.2.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.0.2.conv1x1.bias      	[128]                    	128
	repeats.0.2.nonlinearity1.weight	[1]                      	1
	repeats.0.2.norm1.weight      	[128]                    	128
	repeats.0.2.norm1.bias        	[128]                    	128
	repeats.0.2.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.0.2.depthwise_conv.bias	[128]                    	128
	repeats.0.2.nonlinearity2.weight	[1]                      	1
	repeats.0.2.norm2.weight      	[128]                    	128
	repeats.0.2.norm2.bias        	[128]                    	128
	repeats.0.2.skip_out.weight   	[64, 128, 1]             	8192
	repeats.0.2.skip_out.bias     	[64]                     	64
	repeats.0.2.res_out.weight    	[256, 128, 1]            	32768
	repeats.0.2.res_out.bias      	[256]                    	256
	repeats.0.3.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.0.3.conv1x1.bias      	[128]                    	128
	repeats.0.3.nonlinearity1.weight	[1]                      	1
	repeats.0.3.norm1.weight      	[128]                    	128
	repeats.0.3.norm1.bias        	[128]                    	128
	repeats.0.3.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.0.3.depthwise_conv.bias	[128]                    	128
	repeats.0.3.nonlinearity2.weight	[1]                      	1
	repeats.0.3.norm2.weight      	[128]                    	128
	repeats.0.3.norm2.bias        	[128]                    	128
	repeats.0.3.skip_out.weight   	[64, 128, 1]             	8192
	repeats.0.3.skip_out.bias     	[64]                     	64
	repeats.0.3.res_out.weight    	[256, 128, 1]            	32768
	repeats.0.3.res_out.bias      	[256]                    	256
	repeats.0.4.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.0.4.conv1x1.bias      	[128]                    	128
	repeats.0.4.nonlinearity1.weight	[1]                      	1
	repeats.0.4.norm1.weight      	[128]                    	128
	repeats.0.4.norm1.bias        	[128]                    	128
	repeats.0.4.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.0.4.depthwise_conv.bias	[128]                    	128
	repeats.0.4.nonlinearity2.weight	[1]                      	1
	repeats.0.4.norm2.weight      	[128]                    	128
	repeats.0.4.norm2.bias        	[128]                    	128
	repeats.0.4.skip_out.weight   	[64, 128, 1]             	8192
	repeats.0.4.skip_out.bias     	[64]                     	64
	repeats.0.4.res_out.weight    	[256, 128, 1]            	32768
	repeats.0.4.res_out.bias      	[256]                    	256
	repeats.0.5.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.0.5.conv1x1.bias      	[128]                    	128
	repeats.0.5.nonlinearity1.weight	[1]                      	1
	repeats.0.5.norm1.weight      	[128]                    	128
	repeats.0.5.norm1.bias        	[128]                    	128
	repeats.0.5.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.0.5.depthwise_conv.bias	[128]                    	128
	repeats.0.5.nonlinearity2.weight	[1]                      	1
	repeats.0.5.norm2.weight      	[128]                    	128
	repeats.0.5.norm2.bias        	[128]                    	128
	repeats.0.5.skip_out.weight   	[64, 128, 1]             	8192
	repeats.0.5.skip_out.bias     	[64]                     	64
	repeats.0.5.res_out.weight    	[256, 128, 1]            	32768
	repeats.0.5.res_out.bias      	[256]                    	256
	repeats.1.0.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.1.0.conv1x1.bias      	[128]                    	128
	repeats.1.0.nonlinearity1.weight	[1]                      	1
	repeats.1.0.norm1.weight      	[128]                    	128
	repeats.1.0.norm1.bias        	[128]                    	128
	repeats.1.0.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.1.0.depthwise_conv.bias	[128]                    	128
	repeats.1.0.nonlinearity2.weight	[1]                      	1
	repeats.1.0.norm2.weight      	[128]                    	128
	repeats.1.0.norm2.bias        	[128]                    	128
	repeats.1.0.skip_out.weight   	[64, 128, 1]             	8192
	repeats.1.0.skip_out.bias     	[64]                     	64
	repeats.1.0.res_out.weight    	[256, 128, 1]            	32768
	repeats.1.0.res_out.bias      	[256]                    	256
	repeats.1.1.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.1.1.conv1x1.bias      	[128]                    	128
	repeats.1.1.nonlinearity1.weight	[1]                      	1
	repeats.1.1.norm1.weight      	[128]                    	128
	repeats.1.1.norm1.bias        	[128]                    	128
	repeats.1.1.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.1.1.depthwise_conv.bias	[128]                    	128
	repeats.1.1.nonlinearity2.weight	[1]                      	1
	repeats.1.1.norm2.weight      	[128]                    	128
	repeats.1.1.norm2.bias        	[128]                    	128
	repeats.1.1.skip_out.weight   	[64, 128, 1]             	8192
	repeats.1.1.skip_out.bias     	[64]                     	64
	repeats.1.1.res_out.weight    	[256, 128, 1]            	32768
	repeats.1.1.res_out.bias      	[256]                    	256
	repeats.1.2.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.1.2.conv1x1.bias      	[128]                    	128
	repeats.1.2.nonlinearity1.weight	[1]                      	1
	repeats.1.2.norm1.weight      	[128]                    	128
	repeats.1.2.norm1.bias        	[128]                    	128
	repeats.1.2.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.1.2.depthwise_conv.bias	[128]                    	128
	repeats.1.2.nonlinearity2.weight	[1]                      	1
	repeats.1.2.norm2.weight      	[128]                    	128
	repeats.1.2.norm2.bias        	[128]                    	128
	repeats.1.2.skip_out.weight   	[64, 128, 1]             	8192
	repeats.1.2.skip_out.bias     	[64]                     	64
	repeats.1.2.res_out.weight    	[256, 128, 1]            	32768
	repeats.1.2.res_out.bias      	[256]                    	256
	repeats.1.3.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.1.3.conv1x1.bias      	[128]                    	128
	repeats.1.3.nonlinearity1.weight	[1]                      	1
	repeats.1.3.norm1.weight      	[128]                    	128
	repeats.1.3.norm1.bias        	[128]                    	128
	repeats.1.3.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.1.3.depthwise_conv.bias	[128]                    	128
	repeats.1.3.nonlinearity2.weight	[1]                      	1
	repeats.1.3.norm2.weight      	[128]                    	128
	repeats.1.3.norm2.bias        	[128]                    	128
	repeats.1.3.skip_out.weight   	[64, 128, 1]             	8192
	repeats.1.3.skip_out.bias     	[64]                     	64
	repeats.1.3.res_out.weight    	[256, 128, 1]            	32768
	repeats.1.3.res_out.bias      	[256]                    	256
	repeats.1.4.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.1.4.conv1x1.bias      	[128]                    	128
	repeats.1.4.nonlinearity1.weight	[1]                      	1
	repeats.1.4.norm1.weight      	[128]                    	128
	repeats.1.4.norm1.bias        	[128]                    	128
	repeats.1.4.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.1.4.depthwise_conv.bias	[128]                    	128
	repeats.1.4.nonlinearity2.weight	[1]                      	1
	repeats.1.4.norm2.weight      	[128]                    	128
	repeats.1.4.norm2.bias        	[128]                    	128
	repeats.1.4.skip_out.weight   	[64, 128, 1]             	8192
	repeats.1.4.skip_out.bias     	[64]                     	64
	repeats.1.4.res_out.weight    	[256, 128, 1]            	32768
	repeats.1.4.res_out.bias      	[256]                    	256
	repeats.1.5.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.1.5.conv1x1.bias      	[128]                    	128
	repeats.1.5.nonlinearity1.weight	[1]                      	1
	repeats.1.5.norm1.weight      	[128]                    	128
	repeats.1.5.norm1.bias        	[128]                    	128
	repeats.1.5.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.1.5.depthwise_conv.bias	[128]                    	128
	repeats.1.5.nonlinearity2.weight	[1]                      	1
	repeats.1.5.norm2.weight      	[128]                    	128
	repeats.1.5.norm2.bias        	[128]                    	128
	repeats.1.5.skip_out.weight   	[64, 128, 1]             	8192
	repeats.1.5.skip_out.bias     	[64]                     	64
	repeats.1.5.res_out.weight    	[256, 128, 1]            	32768
	repeats.1.5.res_out.bias      	[256]                    	256
	repeats.2.0.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.2.0.conv1x1.bias      	[128]                    	128
	repeats.2.0.nonlinearity1.weight	[1]                      	1
	repeats.2.0.norm1.weight      	[128]                    	128
	repeats.2.0.norm1.bias        	[128]                    	128
	repeats.2.0.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.2.0.depthwise_conv.bias	[128]                    	128
	repeats.2.0.nonlinearity2.weight	[1]                      	1
	repeats.2.0.norm2.weight      	[128]                    	128
	repeats.2.0.norm2.bias        	[128]                    	128
	repeats.2.0.skip_out.weight   	[64, 128, 1]             	8192
	repeats.2.0.skip_out.bias     	[64]                     	64
	repeats.2.0.res_out.weight    	[256, 128, 1]            	32768
	repeats.2.0.res_out.bias      	[256]                    	256
	repeats.2.1.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.2.1.conv1x1.bias      	[128]                    	128
	repeats.2.1.nonlinearity1.weight	[1]                      	1
	repeats.2.1.norm1.weight      	[128]                    	128
	repeats.2.1.norm1.bias        	[128]                    	128
	repeats.2.1.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.2.1.depthwise_conv.bias	[128]                    	128
	repeats.2.1.nonlinearity2.weight	[1]                      	1
	repeats.2.1.norm2.weight      	[128]                    	128
	repeats.2.1.norm2.bias        	[128]                    	128
	repeats.2.1.skip_out.weight   	[64, 128, 1]             	8192
	repeats.2.1.skip_out.bias     	[64]                     	64
	repeats.2.1.res_out.weight    	[256, 128, 1]            	32768
	repeats.2.1.res_out.bias      	[256]                    	256
	repeats.2.2.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.2.2.conv1x1.bias      	[128]                    	128
	repeats.2.2.nonlinearity1.weight	[1]                      	1
	repeats.2.2.norm1.weight      	[128]                    	128
	repeats.2.2.norm1.bias        	[128]                    	128
	repeats.2.2.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.2.2.depthwise_conv.bias	[128]                    	128
	repeats.2.2.nonlinearity2.weight	[1]                      	1
	repeats.2.2.norm2.weight      	[128]                    	128
	repeats.2.2.norm2.bias        	[128]                    	128
	repeats.2.2.skip_out.weight   	[64, 128, 1]             	8192
	repeats.2.2.skip_out.bias     	[64]                     	64
	repeats.2.2.res_out.weight    	[256, 128, 1]            	32768
	repeats.2.2.res_out.bias      	[256]                    	256
	repeats.2.3.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.2.3.conv1x1.bias      	[128]                    	128
	repeats.2.3.nonlinearity1.weight	[1]                      	1
	repeats.2.3.norm1.weight      	[128]                    	128
	repeats.2.3.norm1.bias        	[128]                    	128
	repeats.2.3.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.2.3.depthwise_conv.bias	[128]                    	128
	repeats.2.3.nonlinearity2.weight	[1]                      	1
	repeats.2.3.norm2.weight      	[128]                    	128
	repeats.2.3.norm2.bias        	[128]                    	128
	repeats.2.3.skip_out.weight   	[64, 128, 1]             	8192
	repeats.2.3.skip_out.bias     	[64]                     	64
	repeats.2.3.res_out.weight    	[256, 128, 1]            	32768
	repeats.2.3.res_out.bias      	[256]                    	256
	repeats.2.4.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.2.4.conv1x1.bias      	[128]                    	128
	repeats.2.4.nonlinearity1.weight	[1]                      	1
	repeats.2.4.norm1.weight      	[128]                    	128
	repeats.2.4.norm1.bias        	[128]                    	128
	repeats.2.4.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.2.4.depthwise_conv.bias	[128]                    	128
	repeats.2.4.nonlinearity2.weight	[1]                      	1
	repeats.2.4.norm2.weight      	[128]                    	128
	repeats.2.4.norm2.bias        	[128]                    	128
	repeats.2.4.skip_out.weight   	[64, 128, 1]             	8192
	repeats.2.4.skip_out.bias     	[64]                     	64
	repeats.2.4.res_out.weight    	[256, 128, 1]            	32768
	repeats.2.4.res_out.bias      	[256]                    	256
	repeats.2.5.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.2.5.conv1x1.bias      	[128]                    	128
	repeats.2.5.nonlinearity1.weight	[1]                      	1
	repeats.2.5.norm1.weight      	[128]                    	128
	repeats.2.5.norm1.bias        	[128]                    	128
	repeats.2.5.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.2.5.depthwise_conv.bias	[128]                    	128
	repeats.2.5.nonlinearity2.weight	[1]                      	1
	repeats.2.5.norm2.weight      	[128]                    	128
	repeats.2.5.norm2.bias        	[128]                    	128
	repeats.2.5.skip_out.weight   	[64, 128, 1]             	8192
	repeats.2.5.skip_out.bias     	[64]                     	64
	repeats.2.5.res_out.weight    	[256, 128, 1]            	32768
	repeats.2.5.res_out.bias      	[256]                    	256
	repeats.3.0.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.3.0.conv1x1.bias      	[128]                    	128
	repeats.3.0.nonlinearity1.weight	[1]                      	1
	repeats.3.0.norm1.weight      	[128]                    	128
	repeats.3.0.norm1.bias        	[128]                    	128
	repeats.3.0.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.3.0.depthwise_conv.bias	[128]                    	128
	repeats.3.0.nonlinearity2.weight	[1]                      	1
	repeats.3.0.norm2.weight      	[128]                    	128
	repeats.3.0.norm2.bias        	[128]                    	128
	repeats.3.0.skip_out.weight   	[64, 128, 1]             	8192
	repeats.3.0.skip_out.bias     	[64]                     	64
	repeats.3.0.res_out.weight    	[256, 128, 1]            	32768
	repeats.3.0.res_out.bias      	[256]                    	256
	repeats.3.1.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.3.1.conv1x1.bias      	[128]                    	128
	repeats.3.1.nonlinearity1.weight	[1]                      	1
	repeats.3.1.norm1.weight      	[128]                    	128
	repeats.3.1.norm1.bias        	[128]                    	128
	repeats.3.1.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.3.1.depthwise_conv.bias	[128]                    	128
	repeats.3.1.nonlinearity2.weight	[1]                      	1
	repeats.3.1.norm2.weight      	[128]                    	128
	repeats.3.1.norm2.bias        	[128]                    	128
	repeats.3.1.skip_out.weight   	[64, 128, 1]             	8192
	repeats.3.1.skip_out.bias     	[64]                     	64
	repeats.3.1.res_out.weight    	[256, 128, 1]            	32768
	repeats.3.1.res_out.bias      	[256]                    	256
	repeats.3.2.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.3.2.conv1x1.bias      	[128]                    	128
	repeats.3.2.nonlinearity1.weight	[1]                      	1
	repeats.3.2.norm1.weight      	[128]                    	128
	repeats.3.2.norm1.bias        	[128]                    	128
	repeats.3.2.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.3.2.depthwise_conv.bias	[128]                    	128
	repeats.3.2.nonlinearity2.weight	[1]                      	1
	repeats.3.2.norm2.weight      	[128]                    	128
	repeats.3.2.norm2.bias        	[128]                    	128
	repeats.3.2.skip_out.weight   	[64, 128, 1]             	8192
	repeats.3.2.skip_out.bias     	[64]                     	64
	repeats.3.2.res_out.weight    	[256, 128, 1]            	32768
	repeats.3.2.res_out.bias      	[256]                    	256
	repeats.3.3.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.3.3.conv1x1.bias      	[128]                    	128
	repeats.3.3.nonlinearity1.weight	[1]                      	1
	repeats.3.3.norm1.weight      	[128]                    	128
	repeats.3.3.norm1.bias        	[128]                    	128
	repeats.3.3.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.3.3.depthwise_conv.bias	[128]                    	128
	repeats.3.3.nonlinearity2.weight	[1]                      	1
	repeats.3.3.norm2.weight      	[128]                    	128
	repeats.3.3.norm2.bias        	[128]                    	128
	repeats.3.3.skip_out.weight   	[64, 128, 1]             	8192
	repeats.3.3.skip_out.bias     	[64]                     	64
	repeats.3.3.res_out.weight    	[256, 128, 1]            	32768
	repeats.3.3.res_out.bias      	[256]                    	256
	repeats.3.4.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.3.4.conv1x1.bias      	[128]                    	128
	repeats.3.4.nonlinearity1.weight	[1]                      	1
	repeats.3.4.norm1.weight      	[128]                    	128
	repeats.3.4.norm1.bias        	[128]                    	128
	repeats.3.4.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.3.4.depthwise_conv.bias	[128]                    	128
	repeats.3.4.nonlinearity2.weight	[1]                      	1
	repeats.3.4.norm2.weight      	[128]                    	128
	repeats.3.4.norm2.bias        	[128]                    	128
	repeats.3.4.skip_out.weight   	[64, 128, 1]             	8192
	repeats.3.4.skip_out.bias     	[64]                     	64
	repeats.3.4.res_out.weight    	[256, 128, 1]            	32768
	repeats.3.4.res_out.bias      	[256]                    	256
	repeats.3.5.conv1x1.weight    	[128, 256, 1]            	32768
	repeats.3.5.conv1x1.bias      	[128]                    	128
	repeats.3.5.nonlinearity1.weight	[1]                      	1
	repeats.3.5.norm1.weight      	[128]                    	128
	repeats.3.5.norm1.bias        	[128]                    	128
	repeats.3.5.depthwise_conv.weight	[128, 1, 3]              	384
	repeats.3.5.depthwise_conv.bias	[128]                    	128
	repeats.3.5.nonlinearity2.weight	[1]                      	1
	repeats.3.5.norm2.weight      	[128]                    	128
	repeats.3.5.norm2.bias        	[128]                    	128
	repeats.3.5.skip_out.weight   	[64, 128, 1]             	8192
	repeats.3.5.skip_out.bias     	[64]                     	64
	repeats.3.5.res_out.weight    	[256, 128, 1]            	32768
	repeats.3.5.res_out.bias      	[256]                    	256
	output.0.weight               	[1]                      	1
	output.1.weight               	[128, 64, 1]             	8192
	output.1.bias                 	[128]                    	128
	conv_1.weight                 	[128, 8, 64]             	65536
	conv_2.0.weight               	[128, 1, 128]            	16384
	blstm.weight_ih_l0            	[256, 128]               	32768
	blstm.weight_hh_l0            	[256, 64]                	16384
	blstm.bias_ih_l0              	[256]                    	256
	blstm.bias_hh_l0              	[256]                    	256
	blstm.weight_ih_l0_reverse    	[256, 128]               	32768
	blstm.weight_hh_l0_reverse    	[256, 64]                	16384
	blstm.bias_ih_l0_reverse      	[256]                    	256
	blstm.bias_hh_l0_reverse      	[256]                    	256
	blstm.weight_ih_l1            	[256, 128]               	32768
	blstm.weight_hh_l1            	[256, 64]                	16384
	blstm.bias_ih_l1              	[256]                    	256
	blstm.bias_hh_l1              	[256]                    	256
	blstm.weight_ih_l1_reverse    	[256, 128]               	32768
	blstm.weight_hh_l1_reverse    	[256, 64]                	16384
	blstm.bias_ih_l1_reverse      	[256]                    	256
	blstm.bias_hh_l1_reverse      	[256]                    	256
	blstm.weight_ih_l2            	[256, 128]               	32768
	blstm.weight_hh_l2            	[256, 64]                	16384
	blstm.bias_ih_l2              	[256]                    	256
	blstm.bias_hh_l2              	[256]                    	256
	blstm.weight_ih_l2_reverse    	[256, 128]               	32768
	blstm.weight_hh_l2_reverse    	[256, 64]                	16384
	blstm.bias_ih_l2_reverse      	[256]                    	256
	blstm.bias_hh_l2_reverse      	[256]                    	256
	se_block.net.0.weight         	[2048, 128]              	262144
	se_block.net.0.bias           	[2048]                   	2048
	se_block.net.2.weight         	[128, 2048]              	262144
	se_block.net.2.bias           	[128]                    	128

	In total this network has 2752817 parameters.

Loading weights for mixture from ../trainings/results/ours_S_pretrained/current_model_for_mixture.params
Starting training with 100 minibatches per epoch ... ...
 [mixture]	Epoch 1 of 311 took 1.82m + 0.07m + 0.01m (finished in 9.8h)	train={loss: 4850.463125, max-gradnorm: nan, reg-term: 0.002475}	valid={stereo_loss mean: 4826.812794, stereo_loss median: 4826.812794}
/home/venv/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /pytorch/aten/src/ATen/native/Convolution.cpp:660.)
  self.padding, self.dilation, self.groups)
/home/venv/lib/python3.7/site-packages/torch/nn/functional.py:627: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
../automix/train.py:872: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  config['GRAD_CLIP_NORM_TYPE']))
 [mixture]	Epoch 2 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.6h)	train={loss: 356.636914, max-gradnorm: nan, reg-term: 0.001865}	valid={stereo_loss mean: 3.813062, stereo_loss median: 3.813062}
 [mixture]	Epoch 3 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.6h)	train={loss: 3.288140, max-gradnorm: nan, reg-term: 0.001562}	valid={stereo_loss mean: 3.180682, stereo_loss median: 3.180682}
 [mixture]	Epoch 4 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.6h)	train={loss: 3.075770, max-gradnorm: 22.421347, reg-term: 0.001367}	valid={stereo_loss mean: 3.046016, stereo_loss median: 3.046016}
 [mixture]	Epoch 5 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.5h)	train={loss: 2.959800, max-gradnorm: nan, reg-term: 0.001219}	valid={stereo_loss mean: 2.907160, stereo_loss median: 2.907160}
 [mixture]	Epoch 6 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.5h)	train={loss: 2.903096, max-gradnorm: nan, reg-term: 0.001095}	valid={stereo_loss mean: 2.859716, stereo_loss median: 2.859716}
 [mixture]	Epoch 7 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.5h)	train={loss: 2.816708, max-gradnorm: 18.445637, reg-term: 0.000988}	valid={stereo_loss mean: 3.099969, stereo_loss median: 3.099969}
 [mixture]	Epoch 8 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.4h)	train={loss: 2.885195, max-gradnorm: 32.628605, reg-term: 0.000893}	valid={stereo_loss mean: 2.844247, stereo_loss median: 2.844247}
 [mixture]	Epoch 9 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.4h)	train={loss: 2.765450, max-gradnorm: 15.312874, reg-term: 0.000811}	valid={stereo_loss mean: 2.765221, stereo_loss median: 2.765221}
 [mixture]	Epoch 10 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.4h)	train={loss: 2.706469, max-gradnorm: 18.239611, reg-term: 0.000740}	valid={stereo_loss mean: 2.735350, stereo_loss median: 2.735350}
 [mixture]	Epoch 11 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.3h)	train={loss: 2.657613, max-gradnorm: 21.305914, reg-term: 0.000676}	valid={stereo_loss mean: 2.808507, stereo_loss median: 2.808507}
 [mixture]	Epoch 12 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.3h)	train={loss: 2.769586, max-gradnorm: 35.942612, reg-term: 0.000619}	valid={stereo_loss mean: 2.716887, stereo_loss median: 2.716887}
 [mixture]	Epoch 13 of 311 took 1.79m + 0.06m + 0.01m (finished in 9.3h)	train={loss: 2.736836, max-gradnorm: nan, reg-term: 0.000570}	valid={stereo_loss mean: 2.685994, stereo_loss median: 2.685994}
 [mixture]	Epoch 14 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.3h)	train={loss: 2.665323, max-gradnorm: nan, reg-term: 0.000526}	valid={stereo_loss mean: 2.720644, stereo_loss median: 2.720644}
 [mixture]	Epoch 15 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.2h)	train={loss: 2.722585, max-gradnorm: 14.698135, reg-term: 0.000486}	valid={stereo_loss mean: 2.670737, stereo_loss median: 2.670737}
 [mixture]	Epoch 16 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.2h)	train={loss: 2.705115, max-gradnorm: 19.075651, reg-term: 0.000451}	valid={stereo_loss mean: 2.671932, stereo_loss median: 2.671932}
 [mixture]	Epoch 17 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.2h)	train={loss: 2.691991, max-gradnorm: 27.777174, reg-term: 0.000419}	valid={stereo_loss mean: 2.666179, stereo_loss median: 2.666179}
 [mixture]	Epoch 18 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.1h)	train={loss: 2.713580, max-gradnorm: 31.262939, reg-term: 0.000391}	valid={stereo_loss mean: 2.775216, stereo_loss median: 2.775216}
 [mixture]	Epoch 19 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.1h)	train={loss: 2.629499, max-gradnorm: nan, reg-term: 0.000365}	valid={stereo_loss mean: 2.701644, stereo_loss median: 2.701644}
 [mixture]	Epoch 20 of 311 took 1.79m + 0.06m + 0.01m (finished in 9.0h)	train={loss: 2.605983, max-gradnorm: 11.689154, reg-term: 0.000342}	valid={stereo_loss mean: 2.660952, stereo_loss median: 2.660952}
 [mixture]	Epoch 21 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.0h)	train={loss: 2.715425, max-gradnorm: 28.228525, reg-term: 0.000320}	valid={stereo_loss mean: 2.790068, stereo_loss median: 2.790068}
 [mixture]	Epoch 22 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.0h)	train={loss: 2.620041, max-gradnorm: 34.954521, reg-term: 0.000302}	valid={stereo_loss mean: 2.710144, stereo_loss median: 2.710144}
 [mixture]	Epoch 23 of 311 took 1.80m + 0.06m + 0.01m (finished in 9.0h)	train={loss: 2.609485, max-gradnorm: 17.574976, reg-term: 0.000285}	valid={stereo_loss mean: 2.608195, stereo_loss median: 2.608195}
 [mixture]	Epoch 24 of 311 took 1.80m + 0.07m + 0.01m (finished in 9.0h)	train={loss: 2.557391, max-gradnorm: 14.822673, reg-term: 0.000269}	valid={stereo_loss mean: 2.619760, stereo_loss median: 2.619760}
 [mixture]	Epoch 25 of 311 took 1.81m + 0.06m + 0.01m (finished in 8.9h)	train={loss: 2.587519, max-gradnorm: nan, reg-term: 0.000256}	valid={stereo_loss mean: 2.650496, stereo_loss median: 2.650496}
 [mixture]	Epoch 26 of 311 took 1.80m + 0.07m + 0.01m (finished in 8.9h)	train={loss: 2.608686, max-gradnorm: nan, reg-term: 0.000243}	valid={stereo_loss mean: 2.669636, stereo_loss median: 2.669636}
 [mixture]	Epoch 27 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.8h)	train={loss: 2.539502, max-gradnorm: 17.326511, reg-term: 0.000232}	valid={stereo_loss mean: 2.629803, stereo_loss median: 2.629803}
 [mixture]	Epoch 28 of 311 took 1.80m + 0.07m + 0.01m (finished in 8.8h)	train={loss: 2.623932, max-gradnorm: 75.264877, reg-term: 0.000222}	valid={stereo_loss mean: 2.599035, stereo_loss median: 2.599035}
 [mixture]	Epoch 29 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.8h)	train={loss: 2.600571, max-gradnorm: 32.610222, reg-term: 0.000212}	valid={stereo_loss mean: 2.618459, stereo_loss median: 2.618459}
 [mixture]	Epoch 30 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.8h)	train={loss: 2.557878, max-gradnorm: nan, reg-term: 0.000204}	valid={stereo_loss mean: 2.637423, stereo_loss median: 2.637423}
 [mixture]	Epoch 31 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.7h)	train={loss: 2.557015, max-gradnorm: 18.993116, reg-term: 0.000196}	valid={stereo_loss mean: 2.585250, stereo_loss median: 2.585250}
 [mixture]	Epoch 32 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.7h)	train={loss: 2.539386, max-gradnorm: 13.268336, reg-term: 0.000189}	valid={stereo_loss mean: 2.581141, stereo_loss median: 2.581141}
 [mixture]	Epoch 33 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.7h)	train={loss: 2.533313, max-gradnorm: 84.347740, reg-term: 0.000183}	valid={stereo_loss mean: 2.589526, stereo_loss median: 2.589526}
 [mixture]	Epoch 34 of 311 took 1.80m + 0.06m + 0.01m (finished in 8.6h)	train={loss: 2.560637, max-gradnorm: 47.025970, reg-term: 0.000177}	valid={stereo_loss mean: 2.621511, stereo_loss median: 2.621511}
 [mixture]	Epoch 35 of 311 took 1.80m + 0.07m + 0.01m (finished in 8.6h)	train={loss: 2.465796, max-gradnorm: nan, reg-term: 0.000172}	valid={stereo_loss mean: 2.589180, stereo_loss median: 2.589180}
 [mixture]	Epoch 36 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.5h)	train={loss: 2.529218, max-gradnorm: nan, reg-term: 0.000168}	valid={stereo_loss mean: 2.658251, stereo_loss median: 2.658251}
 [mixture]	Epoch 37 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.5h)	train={loss: 2.564711, max-gradnorm: 21.154846, reg-term: 0.000163}	valid={stereo_loss mean: 2.562086, stereo_loss median: 2.562086}
 [mixture]	Epoch 38 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.5h)	train={loss: 2.526003, max-gradnorm: 15.290411, reg-term: 0.000160}	valid={stereo_loss mean: 2.558777, stereo_loss median: 2.558777}
 [mixture]	Epoch 39 of 311 took 1.79m + 0.07m + 0.01m (finished in 8.5h)	train={loss: 2.491154, max-gradnorm: 13.547678, reg-term: 0.000157}	valid={stereo_loss mean: 2.538277, stereo_loss median: 2.538277}
 [mixture]	Epoch 40 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.4h)	train={loss: 2.441815, max-gradnorm: 30.164820, reg-term: 0.000154}	valid={stereo_loss mean: 2.585574, stereo_loss median: 2.585574}
 [mixture]	Epoch 41 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.4h)	train={loss: 2.472399, max-gradnorm: 18.368000, reg-term: 0.000151}	valid={stereo_loss mean: 2.612967, stereo_loss median: 2.612967}
 [mixture]	Epoch 42 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.3h)	train={loss: 2.503592, max-gradnorm: 17.203407, reg-term: 0.000149}	valid={stereo_loss mean: 2.543491, stereo_loss median: 2.543491}
 [mixture]	Epoch 43 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.3h)	train={loss: 2.572581, max-gradnorm: nan, reg-term: 0.000147}	valid={stereo_loss mean: 2.529793, stereo_loss median: 2.529793}
 [mixture]	Epoch 44 of 311 took 1.78m + 0.07m + 0.01m (finished in 8.2h)	train={loss: 2.488126, max-gradnorm: 10.486335, reg-term: 0.000146}	valid={stereo_loss mean: 2.517905, stereo_loss median: 2.517905}
 [mixture]	Epoch 45 of 311 took 1.79m + 0.06m + 0.01m (finished in 8.2h)	train={loss: 2.504826, max-gradnorm: 16.568100, reg-term: 0.000144}	valid={stereo_loss mean: 2.540848, stereo_loss median: 2.540848}
 [mixture]	Epoch 46 of 311 took 1.78m + 0.06m + 0.01m (finished in 8.2h)	train={loss: 2.520650, max-gradnorm: nan, reg-term: 0.000143}	valid={stereo_loss mean: 2.535464, stereo_loss median: 2.535464}
 [mixture]	Epoch 47 of 311 took 1.77m + 0.07m + 0.01m (finished in 8.1h)	train={loss: 2.487163, max-gradnorm: 13.401362, reg-term: 0.000142}	valid={stereo_loss mean: 2.577655, stereo_loss median: 2.577655}
 [mixture]	Epoch 48 of 311 took 1.77m + 0.07m + 0.01m (finished in 8.1h)	train={loss: 2.521918, max-gradnorm: 14.813673, reg-term: 0.000141}	valid={stereo_loss mean: 2.565834, stereo_loss median: 2.565834}
 [mixture]	Epoch 49 of 311 took 1.77m + 0.06m + 0.01m (finished in 8.0h)	train={loss: 2.521810, max-gradnorm: nan, reg-term: 0.000141}	valid={stereo_loss mean: 2.553611, stereo_loss median: 2.553611}
 [mixture]	Epoch 50 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.9h)	train={loss: 2.458345, max-gradnorm: 58.271755, reg-term: 0.000140}	valid={stereo_loss mean: 2.543914, stereo_loss median: 2.543914}
 [mixture]	Epoch 51 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.9h)	train={loss: 2.481095, max-gradnorm: 12.193919, reg-term: 0.000140}	valid={stereo_loss mean: 2.596260, stereo_loss median: 2.596260}
 [mixture]	Epoch 52 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.8h)	train={loss: 2.553044, max-gradnorm: 12.976816, reg-term: 0.000140}	valid={stereo_loss mean: 2.599132, stereo_loss median: 2.599132}
 [mixture]	Epoch 53 of 311 took 1.75m + 0.07m + 0.01m (finished in 7.8h)	train={loss: 2.587720, max-gradnorm: 15.278414, reg-term: 0.000140}	valid={stereo_loss mean: 2.598649, stereo_loss median: 2.598649}
 [mixture]	Epoch 54 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.8h)	train={loss: 2.517876, max-gradnorm: 13.659365, reg-term: 0.000140}	valid={stereo_loss mean: 2.579080, stereo_loss median: 2.579080}
 [mixture]	Epoch 55 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.7h)	train={loss: 2.530753, max-gradnorm: 12.025429, reg-term: 0.000140}	valid={stereo_loss mean: 2.648742, stereo_loss median: 2.648742}
 [mixture]	Epoch 56 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.7h)	train={loss: 2.513359, max-gradnorm: 12.823011, reg-term: 0.000140}	valid={stereo_loss mean: 2.560560, stereo_loss median: 2.560560}
 [mixture]	Epoch 57 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.7h)	train={loss: 2.549627, max-gradnorm: nan, reg-term: 0.000140}	valid={stereo_loss mean: 2.558486, stereo_loss median: 2.558486}
 [mixture]	Epoch 58 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.6h)	train={loss: 2.487387, max-gradnorm: 19.041891, reg-term: 0.000140}	valid={stereo_loss mean: 2.611198, stereo_loss median: 2.611198}
 [mixture]	Epoch 59 of 311 took 1.75m + 0.07m + 0.01m (finished in 7.6h)	train={loss: 2.499086, max-gradnorm: 13.419768, reg-term: 0.000140}	valid={stereo_loss mean: 2.581962, stereo_loss median: 2.581962}
 [mixture]	Epoch 60 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.6h)	train={loss: 2.594117, max-gradnorm: 12.212795, reg-term: 0.000141}	valid={stereo_loss mean: 2.608682, stereo_loss median: 2.608682}
 [mixture]	Epoch 61 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.6h)	train={loss: 2.643110, max-gradnorm: nan, reg-term: 0.000141}	valid={stereo_loss mean: 2.638192, stereo_loss median: 2.638192}
 [mixture]	Epoch 62 of 311 took 1.75m + 0.07m + 0.01m (finished in 7.5h)	train={loss: 2.536297, max-gradnorm: 15.156285, reg-term: 0.000142}	valid={stereo_loss mean: 2.599369, stereo_loss median: 2.599369}
 [mixture]	Epoch 63 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.5h)	train={loss: 2.647065, max-gradnorm: 27.816759, reg-term: 0.000142}	valid={stereo_loss mean: 2.642712, stereo_loss median: 2.642712}
 [mixture]	Epoch 64 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.5h)	train={loss: 2.583091, max-gradnorm: 13.383056, reg-term: 0.000143}	valid={stereo_loss mean: 2.594241, stereo_loss median: 2.594241}
 [mixture]	Epoch 65 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.4h)	train={loss: 2.604323, max-gradnorm: 14.930068, reg-term: 0.000143}	valid={stereo_loss mean: 2.656176, stereo_loss median: 2.656176}
 [mixture]	Epoch 66 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.4h)	train={loss: 2.554745, max-gradnorm: 14.070227, reg-term: 0.000144}	valid={stereo_loss mean: 2.613229, stereo_loss median: 2.613229}
 [mixture]	Epoch 67 of 311 took 1.75m + 0.07m + 0.01m (finished in 7.4h)	train={loss: 2.583727, max-gradnorm: 13.967531, reg-term: 0.000145}	valid={stereo_loss mean: 2.605300, stereo_loss median: 2.605300}
 [mixture]	Epoch 68 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.4h)	train={loss: 2.620546, max-gradnorm: 13.122892, reg-term: 0.000146}	valid={stereo_loss mean: 2.606137, stereo_loss median: 2.606137}
 [mixture]	Epoch 69 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.3h)	train={loss: 2.579291, max-gradnorm: 24.516973, reg-term: 0.000147}	valid={stereo_loss mean: 2.635523, stereo_loss median: 2.635523}
 [mixture]	Epoch 70 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.3h)	train={loss: 2.640198, max-gradnorm: 45.152588, reg-term: 0.000147}	valid={stereo_loss mean: 2.603935, stereo_loss median: 2.603935}
 [mixture]	Epoch 71 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.3h)	train={loss: 2.646267, max-gradnorm: 23.325994, reg-term: 0.000148}	valid={stereo_loss mean: 2.611591, stereo_loss median: 2.611591}
 [mixture]	Epoch 72 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.2h)	train={loss: 2.618327, max-gradnorm: 20.239908, reg-term: 0.000149}	valid={stereo_loss mean: 2.603691, stereo_loss median: 2.603691}
 [mixture]	Epoch 73 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.2h)	train={loss: 2.620237, max-gradnorm: 16.794798, reg-term: 0.000149}	valid={stereo_loss mean: 2.614908, stereo_loss median: 2.614908}
 [mixture]	Epoch 74 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.2h)	train={loss: 2.608336, max-gradnorm: 124.673264, reg-term: 0.000150}	valid={stereo_loss mean: 2.612982, stereo_loss median: 2.612982}
 [mixture]	Epoch 75 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.1h)	train={loss: 2.602991, max-gradnorm: 28.705393, reg-term: 0.000151}	valid={stereo_loss mean: 2.634818, stereo_loss median: 2.634818}
 [mixture]	Epoch 76 of 311 took 1.74m + 0.06m + 0.01m (finished in 7.1h)	train={loss: 2.599004, max-gradnorm: 19.092867, reg-term: 0.000152}	valid={stereo_loss mean: 2.613089, stereo_loss median: 2.613089}
 [mixture]	Epoch 77 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.1h)	train={loss: 2.542424, max-gradnorm: 17.546787, reg-term: 0.000153}	valid={stereo_loss mean: 2.624899, stereo_loss median: 2.624899}
 [mixture]	Epoch 78 of 311 took 1.75m + 0.06m + 0.01m (finished in 7.1h)	train={loss: 2.630815, max-gradnorm: 16.865021, reg-term: 0.000153}	valid={stereo_loss mean: 2.637989, stereo_loss median: 2.637989}
 [mixture]	Epoch 79 of 311 took 1.74m + 0.07m + 0.01m (finished in 7.0h)	train={loss: 2.644682, max-gradnorm: 14.596709, reg-term: 0.000154}	valid={stereo_loss mean: 2.627021, stereo_loss median: 2.627021}
 [mixture]	Epoch 80 of 311 took 1.75m + 0.07m + 0.01m (finished in 7.0h)	train={loss: 2.658087, max-gradnorm: 10.947937, reg-term: 0.000155}	valid={stereo_loss mean: 2.692993, stereo_loss median: 2.692993}
 [mixture]	Epoch 81 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.9h)	train={loss: 2.665973, max-gradnorm: nan, reg-term: 0.000155}	valid={stereo_loss mean: 2.637705, stereo_loss median: 2.637705}
 [mixture]	Epoch 82 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.9h)	train={loss: 2.639952, max-gradnorm: 17.295139, reg-term: 0.000156}	valid={stereo_loss mean: 2.631293, stereo_loss median: 2.631293}
 [mixture]	Epoch 83 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.9h)	train={loss: 2.602546, max-gradnorm: 14.889502, reg-term: 0.000157}	valid={stereo_loss mean: 2.642358, stereo_loss median: 2.642358}
 [mixture]	Epoch 84 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.9h)	train={loss: 2.593723, max-gradnorm: 25.551863, reg-term: 0.000158}	valid={stereo_loss mean: 2.631422, stereo_loss median: 2.631422}
 [mixture]	Epoch 85 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.8h)	train={loss: 2.605958, max-gradnorm: 30.149982, reg-term: 0.000159}	valid={stereo_loss mean: 2.671285, stereo_loss median: 2.671285}
 [mixture]	Epoch 86 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.8h)	train={loss: 2.640468, max-gradnorm: 35.474407, reg-term: 0.000160}	valid={stereo_loss mean: 2.747841, stereo_loss median: 2.747841}
 [mixture]	Epoch 87 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.8h)	train={loss: 2.659786, max-gradnorm: 28.517706, reg-term: 0.000161}	valid={stereo_loss mean: 2.681971, stereo_loss median: 2.681971}
 [mixture]	Epoch 88 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.8h)	train={loss: 2.686752, max-gradnorm: 19.593649, reg-term: 0.000161}	valid={stereo_loss mean: 2.646580, stereo_loss median: 2.646580}
 [mixture]	Epoch 89 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.7h)	train={loss: 2.586751, max-gradnorm: 19.465036, reg-term: 0.000162}	valid={stereo_loss mean: 2.661418, stereo_loss median: 2.661418}
 [mixture]	Epoch 90 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.7h)	train={loss: 2.618355, max-gradnorm: 18.088152, reg-term: 0.000163}	valid={stereo_loss mean: 2.649573, stereo_loss median: 2.649573}
 [mixture]	Epoch 91 of 311 took 1.75m + 0.06m + 0.01m (finished in 6.7h)	train={loss: 2.660395, max-gradnorm: 18.809591, reg-term: 0.000164}	valid={stereo_loss mean: 2.642925, stereo_loss median: 2.642925}
 [mixture]	Epoch 92 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.6h)	train={loss: 2.605469, max-gradnorm: 15.994814, reg-term: 0.000165}	valid={stereo_loss mean: 2.623219, stereo_loss median: 2.623219}
 [mixture]	Epoch 93 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.6h)	train={loss: 2.548370, max-gradnorm: nan, reg-term: 0.000166}	valid={stereo_loss mean: 2.670450, stereo_loss median: 2.670450}
 [mixture]	Epoch 94 of 311 took 1.75m + 0.06m + 0.01m (finished in 6.6h)	train={loss: 2.662084, max-gradnorm: 31.063019, reg-term: 0.000166}	valid={stereo_loss mean: 2.655531, stereo_loss median: 2.655531}
 [mixture]	Epoch 95 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.6h)	train={loss: 2.675062, max-gradnorm: 44.354313, reg-term: 0.000167}	valid={stereo_loss mean: 2.687669, stereo_loss median: 2.687669}
 [mixture]	Epoch 96 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.5h)	train={loss: 2.621100, max-gradnorm: 27.681261, reg-term: 0.000168}	valid={stereo_loss mean: 2.666495, stereo_loss median: 2.666495}
 [mixture]	Epoch 97 of 311 took 1.75m + 0.06m + 0.01m (finished in 6.5h)	train={loss: 2.671267, max-gradnorm: 140.963181, reg-term: 0.000168}	valid={stereo_loss mean: 2.666388, stereo_loss median: 2.666388}
 [mixture]	Epoch 98 of 311 took 1.75m + 0.06m + 0.01m (finished in 6.5h)	train={loss: 2.650816, max-gradnorm: 19.650743, reg-term: 0.000169}	valid={stereo_loss mean: 2.665248, stereo_loss median: 2.665248}
 [mixture]	Epoch 99 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.4h)	train={loss: 2.661564, max-gradnorm: nan, reg-term: 0.000169}	valid={stereo_loss mean: 2.773940, stereo_loss median: 2.773940}
 [mixture]	Epoch 100 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.4h)	train={loss: 2.733034, max-gradnorm: 25.052376, reg-term: 0.000170}	valid={stereo_loss mean: 2.698196, stereo_loss median: 2.698196}
 [mixture]	Epoch 101 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.3h)	train={loss: 2.702538, max-gradnorm: 24.561087, reg-term: 0.000170}	valid={stereo_loss mean: 2.698468, stereo_loss median: 2.698468}
 [mixture]	Epoch 102 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.3h)	train={loss: 2.686628, max-gradnorm: 22.620235, reg-term: 0.000170}	valid={stereo_loss mean: 2.700384, stereo_loss median: 2.700384}
 [mixture]	Epoch 103 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.3h)	train={loss: 2.705721, max-gradnorm: 25.613489, reg-term: 0.000171}	valid={stereo_loss mean: 2.692269, stereo_loss median: 2.692269}
 [mixture]	Epoch 104 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.2h)	train={loss: 2.611893, max-gradnorm: 23.406908, reg-term: 0.000171}	valid={stereo_loss mean: 2.706036, stereo_loss median: 2.706036}
 [mixture]	Epoch 105 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.2h)	train={loss: 2.661667, max-gradnorm: 27.048706, reg-term: 0.000172}	valid={stereo_loss mean: 2.672583, stereo_loss median: 2.672583}
 [mixture]	Epoch 106 of 311 took 1.74m + 0.06m + 0.01m (finished in 6.2h)	train={loss: 2.629049, max-gradnorm: 23.328236, reg-term: 0.000172}	valid={stereo_loss mean: 2.695937, stereo_loss median: 2.695937}
 [mixture]	Epoch 107 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.2h)	train={loss: 2.705006, max-gradnorm: 25.708374, reg-term: 0.000172}	valid={stereo_loss mean: 2.694120, stereo_loss median: 2.694120}
 [mixture]	Epoch 108 of 311 took 1.75m + 0.07m + 0.01m (finished in 6.1h)	train={loss: 2.716706, max-gradnorm: 31.588402, reg-term: 0.000173}	valid={stereo_loss mean: 2.706374, stereo_loss median: 2.706374}
 [mixture]	Epoch 109 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.1h)	train={loss: 2.708430, max-gradnorm: 25.373095, reg-term: 0.000173}	valid={stereo_loss mean: 2.682341, stereo_loss median: 2.682341}
 [mixture]	Epoch 110 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.1h)	train={loss: 2.615338, max-gradnorm: 37.155945, reg-term: 0.000173}	valid={stereo_loss mean: 2.692731, stereo_loss median: 2.692731}
 [mixture]	Epoch 111 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.0h)	train={loss: 2.642686, max-gradnorm: 26.058744, reg-term: 0.000173}	valid={stereo_loss mean: 2.693786, stereo_loss median: 2.693786}
 [mixture]	Epoch 112 of 311 took 1.74m + 0.07m + 0.01m (finished in 6.0h)	train={loss: 2.732730, max-gradnorm: 48.407841, reg-term: 0.000174}	valid={stereo_loss mean: 2.739994, stereo_loss median: 2.739994}
 [mixture]	Epoch 113 of 311 took 1.75m + 0.06m + 0.01m (finished in 6.0h)	train={loss: 2.618517, max-gradnorm: 30.383894, reg-term: 0.000175}	valid={stereo_loss mean: 2.691453, stereo_loss median: 2.691453}
 [mixture]	Epoch 114 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.9h)	train={loss: 2.634850, max-gradnorm: 28.451233, reg-term: 0.000175}	valid={stereo_loss mean: 2.708624, stereo_loss median: 2.708624}
 [mixture]	Epoch 115 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.9h)	train={loss: 2.674585, max-gradnorm: 24.690022, reg-term: 0.000175}	valid={stereo_loss mean: 2.732279, stereo_loss median: 2.732279}
 [mixture]	Epoch 116 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.9h)	train={loss: 2.722100, max-gradnorm: 24.191488, reg-term: 0.000176}	valid={stereo_loss mean: 2.688036, stereo_loss median: 2.688036}
 [mixture]	Epoch 117 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.8h)	train={loss: 2.666003, max-gradnorm: 23.081385, reg-term: 0.000177}	valid={stereo_loss mean: 2.709742, stereo_loss median: 2.709742}
 [mixture]	Epoch 118 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.8h)	train={loss: 2.675881, max-gradnorm: 35.992615, reg-term: 0.000178}	valid={stereo_loss mean: 2.708963, stereo_loss median: 2.708963}
 [mixture]	Epoch 119 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.8h)	train={loss: 2.650568, max-gradnorm: 36.400379, reg-term: 0.000180}	valid={stereo_loss mean: 2.714145, stereo_loss median: 2.714145}
 [mixture]	Epoch 120 of 311 took 1.74m + 0.07m + 0.01m (finished in 5.8h)	train={loss: 2.682373, max-gradnorm: 21.923622, reg-term: 0.000181}	valid={stereo_loss mean: 2.693319, stereo_loss median: 2.693319}
 [mixture]	Epoch 121 of 311 took 1.74m + 0.07m + 0.01m (finished in 5.7h)	train={loss: 2.693752, max-gradnorm: 28.792204, reg-term: 0.000182}	valid={stereo_loss mean: 2.671640, stereo_loss median: 2.671640}
 [mixture]	Epoch 122 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.7h)	train={loss: 2.643795, max-gradnorm: 28.335308, reg-term: 0.000184}	valid={stereo_loss mean: 2.660961, stereo_loss median: 2.660961}
 [mixture]	Epoch 123 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.7h)	train={loss: 2.674258, max-gradnorm: 40.903774, reg-term: 0.000187}	valid={stereo_loss mean: 2.684248, stereo_loss median: 2.684248}
 [mixture]	Epoch 124 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.7h)	train={loss: 2.681215, max-gradnorm: 37.163074, reg-term: 0.000187}	valid={stereo_loss mean: 2.673334, stereo_loss median: 2.673334}
 [mixture]	Epoch 125 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.6h)	train={loss: 2.678036, max-gradnorm: 24.022781, reg-term: 0.000190}	valid={stereo_loss mean: 2.677819, stereo_loss median: 2.677819}
 [mixture]	Epoch 126 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.6h)	train={loss: 2.626234, max-gradnorm: 32.901550, reg-term: 0.000191}	valid={stereo_loss mean: 2.684489, stereo_loss median: 2.684489}
 [mixture]	Epoch 127 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.6h)	train={loss: 2.602192, max-gradnorm: 25.760679, reg-term: 0.000192}	valid={stereo_loss mean: 2.752756, stereo_loss median: 2.752756}
 [mixture]	Epoch 128 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.5h)	train={loss: 2.651060, max-gradnorm: 35.069382, reg-term: 0.000193}	valid={stereo_loss mean: 2.663698, stereo_loss median: 2.663698}
 [mixture]	Epoch 129 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.5h)	train={loss: 2.603506, max-gradnorm: 25.071735, reg-term: 0.000193}	valid={stereo_loss mean: 2.684477, stereo_loss median: 2.684477}
 [mixture]	Epoch 130 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.5h)	train={loss: 2.661147, max-gradnorm: 27.778986, reg-term: 0.000194}	valid={stereo_loss mean: 2.668550, stereo_loss median: 2.668550}
 [mixture]	Epoch 131 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.4h)	train={loss: 2.650030, max-gradnorm: 30.461483, reg-term: 0.000194}	valid={stereo_loss mean: 2.655700, stereo_loss median: 2.655700}
 [mixture]	Epoch 132 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.4h)	train={loss: 2.641237, max-gradnorm: 22.357880, reg-term: 0.000195}	valid={stereo_loss mean: 2.677451, stereo_loss median: 2.677451}
 [mixture]	Epoch 133 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.4h)	train={loss: 2.631092, max-gradnorm: 36.205780, reg-term: 0.000195}	valid={stereo_loss mean: 2.659645, stereo_loss median: 2.659645}
 [mixture]	Epoch 134 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.4h)	train={loss: 2.651848, max-gradnorm: 39.697506, reg-term: 0.000197}	valid={stereo_loss mean: 2.774850, stereo_loss median: 2.774850}
 [mixture]	Epoch 135 of 311 took 1.74m + 0.07m + 0.01m (finished in 5.3h)	train={loss: 2.720585, max-gradnorm: 26.585350, reg-term: 0.000198}	valid={stereo_loss mean: 2.649670, stereo_loss median: 2.649670}
 [mixture]	Epoch 136 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.3h)	train={loss: 2.592954, max-gradnorm: 29.486126, reg-term: 0.000201}	valid={stereo_loss mean: 2.688340, stereo_loss median: 2.688340}
 [mixture]	Epoch 137 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.3h)	train={loss: 2.646689, max-gradnorm: 28.151901, reg-term: 0.000200}	valid={stereo_loss mean: 2.669369, stereo_loss median: 2.669369}
 [mixture]	Epoch 138 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.2h)	train={loss: 2.673514, max-gradnorm: 31.784365, reg-term: 0.000201}	valid={stereo_loss mean: 2.676332, stereo_loss median: 2.676332}
 [mixture]	Epoch 139 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.2h)	train={loss: 2.628661, max-gradnorm: 244.178635, reg-term: 0.000202}	valid={stereo_loss mean: 2.769518, stereo_loss median: 2.769518}
 [mixture]	Epoch 140 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.2h)	train={loss: 2.611881, max-gradnorm: 27.132706, reg-term: 0.000202}	valid={stereo_loss mean: 2.686154, stereo_loss median: 2.686154}
 [mixture]	Epoch 141 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.1h)	train={loss: 2.576714, max-gradnorm: nan, reg-term: 0.000202}	valid={stereo_loss mean: 2.775240, stereo_loss median: 2.775240}
 [mixture]	Epoch 142 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.1h)	train={loss: 2.670382, max-gradnorm: 39.200073, reg-term: 0.000203}	valid={stereo_loss mean: 2.696920, stereo_loss median: 2.696920}
 [mixture]	Epoch 143 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.1h)	train={loss: 2.679806, max-gradnorm: 27.080978, reg-term: 0.000203}	valid={stereo_loss mean: 2.691901, stereo_loss median: 2.691901}
 [mixture]	Epoch 144 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.1h)	train={loss: 2.608261, max-gradnorm: 22.043789, reg-term: 0.000205}	valid={stereo_loss mean: 2.708610, stereo_loss median: 2.708610}
 [mixture]	Epoch 145 of 311 took 1.75m + 0.07m + 0.01m (finished in 5.0h)	train={loss: 2.634740, max-gradnorm: 47.266743, reg-term: 0.000205}	valid={stereo_loss mean: 2.679441, stereo_loss median: 2.679441}
 [mixture]	Epoch 146 of 311 took 1.75m + 0.06m + 0.01m (finished in 5.0h)	train={loss: 2.556989, max-gradnorm: 38.345879, reg-term: 0.000205}	valid={stereo_loss mean: 2.694899, stereo_loss median: 2.694899}
 [mixture]	Epoch 147 of 311 took 1.74m + 0.06m + 0.01m (finished in 5.0h)	train={loss: 2.624668, max-gradnorm: 26.691128, reg-term: 0.000206}	valid={stereo_loss mean: 2.673168, stereo_loss median: 2.673168}
 [mixture]	Epoch 148 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.9h)	train={loss: 2.640073, max-gradnorm: 28.493059, reg-term: 0.000207}	valid={stereo_loss mean: 2.679347, stereo_loss median: 2.679347}
 [mixture]	Epoch 149 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.9h)	train={loss: 2.683333, max-gradnorm: 23.635370, reg-term: 0.000208}	valid={stereo_loss mean: 2.667917, stereo_loss median: 2.667917}
 [mixture]	Epoch 150 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.9h)	train={loss: 2.691783, max-gradnorm: 26.048977, reg-term: 0.000209}	valid={stereo_loss mean: 2.666939, stereo_loss median: 2.666939}
 [mixture]	Epoch 151 of 311 took 1.75m + 0.07m + 0.01m (finished in 4.9h)	train={loss: 2.687228, max-gradnorm: 22.280005, reg-term: 0.000211}	valid={stereo_loss mean: 2.685347, stereo_loss median: 2.685347}
 [mixture]	Epoch 152 of 311 took 1.75m + 0.07m + 0.01m (finished in 4.8h)	train={loss: 2.590121, max-gradnorm: 16.924992, reg-term: 0.000211}	valid={stereo_loss mean: 2.657205, stereo_loss median: 2.657205}
 [mixture]	Epoch 153 of 311 took 1.76m + 0.07m + 0.01m (finished in 4.8h)	train={loss: 2.680209, max-gradnorm: 31.645178, reg-term: 0.000212}	valid={stereo_loss mean: 2.656854, stereo_loss median: 2.656854}
 [mixture]	Epoch 154 of 311 took 1.75m + 0.06m + 0.01m (finished in 4.8h)	train={loss: 2.644167, max-gradnorm: 24.948881, reg-term: 0.000212}	valid={stereo_loss mean: 2.658336, stereo_loss median: 2.658336}
 [mixture]	Epoch 155 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.7h)	train={loss: 2.600331, max-gradnorm: 35.387978, reg-term: 0.000212}	valid={stereo_loss mean: 2.656489, stereo_loss median: 2.656489}
 [mixture]	Epoch 156 of 311 took 1.75m + 0.06m + 0.01m (finished in 4.7h)	train={loss: 2.638857, max-gradnorm: 31.824902, reg-term: 0.000212}	valid={stereo_loss mean: 2.659530, stereo_loss median: 2.659530}
 [mixture]	Epoch 157 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.7h)	train={loss: 2.570795, max-gradnorm: 27.314716, reg-term: 0.000212}	valid={stereo_loss mean: 2.664145, stereo_loss median: 2.664145}
 [mixture]	Epoch 158 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.6h)	train={loss: 2.615063, max-gradnorm: 29.814432, reg-term: 0.000212}	valid={stereo_loss mean: 2.653563, stereo_loss median: 2.653563}
 [mixture]	Epoch 159 of 311 took 1.75m + 0.07m + 0.01m (finished in 4.6h)	train={loss: 2.604376, max-gradnorm: 17.167196, reg-term: 0.000213}	valid={stereo_loss mean: 2.660090, stereo_loss median: 2.660090}
 [mixture]	Epoch 160 of 311 took 1.75m + 0.07m + 0.01m (finished in 4.6h)	train={loss: 2.524707, max-gradnorm: 15.916983, reg-term: 0.000212}	valid={stereo_loss mean: 2.658197, stereo_loss median: 2.658197}
 [mixture]	Epoch 161 of 311 took 1.75m + 0.06m + 0.01m (finished in 4.5h)	train={loss: 2.563416, max-gradnorm: nan, reg-term: 0.000212}	valid={stereo_loss mean: 2.657460, stereo_loss median: 2.657460}
 [mixture]	Epoch 162 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.5h)	train={loss: 2.615405, max-gradnorm: 23.330544, reg-term: 0.000212}	valid={stereo_loss mean: 2.657830, stereo_loss median: 2.657830}
 [mixture]	Epoch 163 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.5h)	train={loss: 2.640115, max-gradnorm: 20.613382, reg-term: 0.000212}	valid={stereo_loss mean: 2.661330, stereo_loss median: 2.661330}
 [mixture]	Epoch 164 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.4h)	train={loss: 2.596852, max-gradnorm: 19.824455, reg-term: 0.000212}	valid={stereo_loss mean: 2.668023, stereo_loss median: 2.668023}
 [mixture]	Epoch 165 of 311 took 1.75m + 0.06m + 0.01m (finished in 4.4h)	train={loss: 2.588405, max-gradnorm: 26.351986, reg-term: 0.000212}	valid={stereo_loss mean: 2.672427, stereo_loss median: 2.672427}
 [mixture]	Epoch 166 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.4h)	train={loss: 2.668034, max-gradnorm: 27.443157, reg-term: 0.000212}	valid={stereo_loss mean: 2.665936, stereo_loss median: 2.665936}
 [mixture]	Epoch 167 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.3h)	train={loss: 2.706846, max-gradnorm: 24.816799, reg-term: 0.000212}	valid={stereo_loss mean: 2.660013, stereo_loss median: 2.660013}
 [mixture]	Epoch 168 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.3h)	train={loss: 2.570203, max-gradnorm: 17.461346, reg-term: 0.000212}	valid={stereo_loss mean: 2.652513, stereo_loss median: 2.652513}
 [mixture]	Epoch 169 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.3h)	train={loss: 2.621630, max-gradnorm: 18.635275, reg-term: 0.000212}	valid={stereo_loss mean: 2.667663, stereo_loss median: 2.667663}
 [mixture]	Epoch 170 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.3h)	train={loss: 2.643234, max-gradnorm: 35.800625, reg-term: 0.000212}	valid={stereo_loss mean: 2.660736, stereo_loss median: 2.660736}
 [mixture]	Epoch 171 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.2h)	train={loss: 2.645690, max-gradnorm: 18.851288, reg-term: 0.000212}	valid={stereo_loss mean: 2.658069, stereo_loss median: 2.658069}
 [mixture]	Epoch 172 of 311 took 1.75m + 0.06m + 0.01m (finished in 4.2h)	train={loss: 2.613548, max-gradnorm: 26.988068, reg-term: 0.000212}	valid={stereo_loss mean: 2.658471, stereo_loss median: 2.658471}
 [mixture]	Epoch 173 of 311 took 1.75m + 0.06m + 0.01m (finished in 4.2h)	train={loss: 2.652022, max-gradnorm: 28.191933, reg-term: 0.000212}	valid={stereo_loss mean: 2.664723, stereo_loss median: 2.664723}
 [mixture]	Epoch 174 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.1h)	train={loss: 2.606833, max-gradnorm: 31.130981, reg-term: 0.000212}	valid={stereo_loss mean: 2.655579, stereo_loss median: 2.655579}
 [mixture]	Epoch 175 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.1h)	train={loss: 2.616139, max-gradnorm: 25.638226, reg-term: 0.000213}	valid={stereo_loss mean: 2.662675, stereo_loss median: 2.662675}
 [mixture]	Epoch 176 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.1h)	train={loss: 2.591302, max-gradnorm: 22.294897, reg-term: 0.000213}	valid={stereo_loss mean: 2.662232, stereo_loss median: 2.662232}
 [mixture]	Epoch 177 of 311 took 1.74m + 0.07m + 0.01m (finished in 4.0h)	train={loss: 2.575546, max-gradnorm: 37.060520, reg-term: 0.000213}	valid={stereo_loss mean: 2.668572, stereo_loss median: 2.668572}
 [mixture]	Epoch 178 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.0h)	train={loss: 2.614294, max-gradnorm: 19.109470, reg-term: 0.000213}	valid={stereo_loss mean: 2.662203, stereo_loss median: 2.662203}
 [mixture]	Epoch 179 of 311 took 1.74m + 0.06m + 0.01m (finished in 4.0h)	train={loss: 2.639798, max-gradnorm: 93.961121, reg-term: 0.000213}	valid={stereo_loss mean: 2.663258, stereo_loss median: 2.663258}
 [mixture]	Epoch 180 of 311 took 1.75m + 0.07m + 0.01m (finished in 4.0h)	train={loss: 2.597542, max-gradnorm: 23.042683, reg-term: 0.000213}	valid={stereo_loss mean: 2.652672, stereo_loss median: 2.652672}
 [mixture]	Epoch 181 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.9h)	train={loss: 2.622866, max-gradnorm: 77.238304, reg-term: 0.000213}	valid={stereo_loss mean: 2.655253, stereo_loss median: 2.655253}
 [mixture]	Epoch 182 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.9h)	train={loss: 2.659535, max-gradnorm: 16.304554, reg-term: 0.000213}	valid={stereo_loss mean: 2.658173, stereo_loss median: 2.658173}
 [mixture]	Epoch 183 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.9h)	train={loss: 2.649736, max-gradnorm: 84.204788, reg-term: 0.000213}	valid={stereo_loss mean: 2.656006, stereo_loss median: 2.656006}
 [mixture]	Epoch 184 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.8h)	train={loss: 2.635409, max-gradnorm: 14.445890, reg-term: 0.000213}	valid={stereo_loss mean: 2.674874, stereo_loss median: 2.674874}
 [mixture]	Epoch 185 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.8h)	train={loss: 2.564826, max-gradnorm: 422.618011, reg-term: 0.000213}	valid={stereo_loss mean: 2.657200, stereo_loss median: 2.657200}
 [mixture]	Epoch 186 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.8h)	train={loss: 2.647504, max-gradnorm: 16.832935, reg-term: 0.000213}	valid={stereo_loss mean: 2.663077, stereo_loss median: 2.663077}
 [mixture]	Epoch 187 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.8h)	train={loss: 2.572937, max-gradnorm: 20.265352, reg-term: 0.000213}	valid={stereo_loss mean: 2.664678, stereo_loss median: 2.664678}
 [mixture]	Epoch 188 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.7h)	train={loss: 2.597198, max-gradnorm: 15.670934, reg-term: 0.000213}	valid={stereo_loss mean: 2.659635, stereo_loss median: 2.659635}
 [mixture]	Epoch 189 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.7h)	train={loss: 2.606383, max-gradnorm: 20.237627, reg-term: 0.000214}	valid={stereo_loss mean: 2.670295, stereo_loss median: 2.670295}
 [mixture]	Epoch 190 of 311 took 1.74m + 0.07m + 0.01m (finished in 3.7h)	train={loss: 2.607647, max-gradnorm: 24.263338, reg-term: 0.000214}	valid={stereo_loss mean: 2.657051, stereo_loss median: 2.657051}
 [mixture]	Epoch 191 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.6h)	train={loss: 2.624641, max-gradnorm: 17.512871, reg-term: 0.000214}	valid={stereo_loss mean: 2.648285, stereo_loss median: 2.648285}
 [mixture]	Epoch 192 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.6h)	train={loss: 2.627856, max-gradnorm: 16.192348, reg-term: 0.000214}	valid={stereo_loss mean: 2.653419, stereo_loss median: 2.653419}
 [mixture]	Epoch 193 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.6h)	train={loss: 2.590580, max-gradnorm: 17.581776, reg-term: 0.000214}	valid={stereo_loss mean: 2.661828, stereo_loss median: 2.661828}
 [mixture]	Epoch 194 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.6h)	train={loss: 2.599375, max-gradnorm: 28.889816, reg-term: 0.000214}	valid={stereo_loss mean: 2.647295, stereo_loss median: 2.647295}
 [mixture]	Epoch 195 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.5h)	train={loss: 2.602154, max-gradnorm: 20.626850, reg-term: 0.000214}	valid={stereo_loss mean: 2.650302, stereo_loss median: 2.650302}
 [mixture]	Epoch 196 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.5h)	train={loss: 2.679308, max-gradnorm: 46.019833, reg-term: 0.000215}	valid={stereo_loss mean: 2.650423, stereo_loss median: 2.650423}
 [mixture]	Epoch 197 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.5h)	train={loss: 2.640825, max-gradnorm: 16.544544, reg-term: 0.000215}	valid={stereo_loss mean: 2.653487, stereo_loss median: 2.653487}
 [mixture]	Epoch 198 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.4h)	train={loss: 2.635200, max-gradnorm: 17.992245, reg-term: 0.000215}	valid={stereo_loss mean: 2.651815, stereo_loss median: 2.651815}
 [mixture]	Epoch 199 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.4h)	train={loss: 2.575500, max-gradnorm: 14.318634, reg-term: 0.000215}	valid={stereo_loss mean: 2.652765, stereo_loss median: 2.652765}
 [mixture]	Epoch 200 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.4h)	train={loss: 2.528916, max-gradnorm: 30.792852, reg-term: 0.000215}	valid={stereo_loss mean: 2.657832, stereo_loss median: 2.657832}
 [mixture]	Epoch 201 of 311 took 1.74m + 0.07m + 0.01m (finished in 3.3h)	train={loss: 2.567389, max-gradnorm: 40.768501, reg-term: 0.000216}	valid={stereo_loss mean: 2.657579, stereo_loss median: 2.657579}
 [mixture]	Epoch 202 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.3h)	train={loss: 2.605751, max-gradnorm: 13.929369, reg-term: 0.000216}	valid={stereo_loss mean: 2.640744, stereo_loss median: 2.640744}
 [mixture]	Epoch 203 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.3h)	train={loss: 2.504594, max-gradnorm: 10.748081, reg-term: 0.000216}	valid={stereo_loss mean: 2.643533, stereo_loss median: 2.643533}
 [mixture]	Epoch 204 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.2h)	train={loss: 2.639369, max-gradnorm: 13.121174, reg-term: 0.000216}	valid={stereo_loss mean: 2.640314, stereo_loss median: 2.640314}
 [mixture]	Epoch 205 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.2h)	train={loss: 2.649424, max-gradnorm: 22.020885, reg-term: 0.000216}	valid={stereo_loss mean: 2.640873, stereo_loss median: 2.640873}
 [mixture]	Epoch 206 of 311 took 1.74m + 0.06m + 0.01m (finished in 3.2h)	train={loss: 2.560979, max-gradnorm: 21.144464, reg-term: 0.000216}	valid={stereo_loss mean: 2.640077, stereo_loss median: 2.640077}
 [mixture]	Epoch 207 of 311 took 1.74m + 0.07m + 0.01m (finished in 3.1h)	train={loss: 2.625742, max-gradnorm: 12.235029, reg-term: 0.000216}	valid={stereo_loss mean: 2.641641, stereo_loss median: 2.641641}
 [mixture]	Epoch 208 of 311 took 1.74m + 0.07m + 0.01m (finished in 3.1h)	train={loss: 2.623049, max-gradnorm: 57.754856, reg-term: 0.000216}	valid={stereo_loss mean: 2.641847, stereo_loss median: 2.641847}
 [mixture]	Epoch 209 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.1h)	train={loss: 2.574155, max-gradnorm: 268.335175, reg-term: 0.000216}	valid={stereo_loss mean: 2.638713, stereo_loss median: 2.638713}
 [mixture]	Epoch 210 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.1h)	train={loss: 2.592888, max-gradnorm: 19.302618, reg-term: 0.000217}	valid={stereo_loss mean: 2.644608, stereo_loss median: 2.644608}
 [mixture]	Epoch 211 of 311 took 1.75m + 0.07m + 0.01m (finished in 3.0h)	train={loss: 2.548034, max-gradnorm: 13.633898, reg-term: 0.000217}	valid={stereo_loss mean: 2.647107, stereo_loss median: 2.647107}
 [mixture]	Epoch 212 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.0h)	train={loss: 2.611188, max-gradnorm: 13.336873, reg-term: 0.000217}	valid={stereo_loss mean: 2.642790, stereo_loss median: 2.642790}
 [mixture]	Epoch 213 of 311 took 1.75m + 0.06m + 0.01m (finished in 3.0h)	train={loss: 2.558997, max-gradnorm: 13.740348, reg-term: 0.000217}	valid={stereo_loss mean: 2.642099, stereo_loss median: 2.642099}
 [mixture]	Epoch 214 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.9h)	train={loss: 2.547295, max-gradnorm: 59.540222, reg-term: 0.000217}	valid={stereo_loss mean: 2.638800, stereo_loss median: 2.638800}
 [mixture]	Epoch 215 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.9h)	train={loss: 2.617312, max-gradnorm: 11.521742, reg-term: 0.000217}	valid={stereo_loss mean: 2.638516, stereo_loss median: 2.638516}
 [mixture]	Epoch 216 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.9h)	train={loss: 2.629696, max-gradnorm: 23.027279, reg-term: 0.000217}	valid={stereo_loss mean: 2.637986, stereo_loss median: 2.637986}
 [mixture]	Epoch 217 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.9h)	train={loss: 2.620819, max-gradnorm: 18.328255, reg-term: 0.000217}	valid={stereo_loss mean: 2.644889, stereo_loss median: 2.644889}
 [mixture]	Epoch 218 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.8h)	train={loss: 2.624998, max-gradnorm: 23.795557, reg-term: 0.000217}	valid={stereo_loss mean: 2.636540, stereo_loss median: 2.636540}
 [mixture]	Epoch 219 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.8h)	train={loss: 2.535873, max-gradnorm: 12.785228, reg-term: 0.000217}	valid={stereo_loss mean: 2.642368, stereo_loss median: 2.642368}
 [mixture]	Epoch 220 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.8h)	train={loss: 2.602607, max-gradnorm: 10.766899, reg-term: 0.000217}	valid={stereo_loss mean: 2.638780, stereo_loss median: 2.638780}
 [mixture]	Epoch 221 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.7h)	train={loss: 2.505185, max-gradnorm: 8.692140, reg-term: 0.000217}	valid={stereo_loss mean: 2.637830, stereo_loss median: 2.637830}
 [mixture]	Epoch 222 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.7h)	train={loss: 2.588130, max-gradnorm: 8.781418, reg-term: 0.000217}	valid={stereo_loss mean: 2.638007, stereo_loss median: 2.638007}
 [mixture]	Epoch 223 of 311 took 1.74m + 0.07m + 0.01m (finished in 2.7h)	train={loss: 2.746231, max-gradnorm: 267.028137, reg-term: 0.000217}	valid={stereo_loss mean: 2.636287, stereo_loss median: 2.636287}
 [mixture]	Epoch 224 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.6h)	train={loss: 2.634419, max-gradnorm: 21.292921, reg-term: 0.000217}	valid={stereo_loss mean: 2.640105, stereo_loss median: 2.640105}
 [mixture]	Epoch 225 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.6h)	train={loss: 2.527870, max-gradnorm: 12.467663, reg-term: 0.000217}	valid={stereo_loss mean: 2.640191, stereo_loss median: 2.640191}
 [mixture]	Epoch 226 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.6h)	train={loss: 2.558123, max-gradnorm: 13.125715, reg-term: 0.000218}	valid={stereo_loss mean: 2.638275, stereo_loss median: 2.638275}
 [mixture]	Epoch 227 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.5h)	train={loss: 2.569327, max-gradnorm: 10.890582, reg-term: 0.000218}	valid={stereo_loss mean: 2.637994, stereo_loss median: 2.637994}
 [mixture]	Epoch 228 of 311 took 1.74m + 0.07m + 0.01m (finished in 2.5h)	train={loss: 2.617809, max-gradnorm: 8.386447, reg-term: 0.000218}	valid={stereo_loss mean: 2.641033, stereo_loss median: 2.641033}
 [mixture]	Epoch 229 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.5h)	train={loss: 2.635662, max-gradnorm: 10.968736, reg-term: 0.000218}	valid={stereo_loss mean: 2.637787, stereo_loss median: 2.637787}
 [mixture]	Epoch 230 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.5h)	train={loss: 2.556410, max-gradnorm: 10.124081, reg-term: 0.000218}	valid={stereo_loss mean: 2.639342, stereo_loss median: 2.639342}
 [mixture]	Epoch 231 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.4h)	train={loss: 2.588703, max-gradnorm: 10.461513, reg-term: 0.000218}	valid={stereo_loss mean: 2.638774, stereo_loss median: 2.638774}
 [mixture]	Epoch 232 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.4h)	train={loss: 2.549316, max-gradnorm: 10.524195, reg-term: 0.000219}	valid={stereo_loss mean: 2.637298, stereo_loss median: 2.637298}
 [mixture]	Epoch 233 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.4h)	train={loss: 2.610043, max-gradnorm: 7.020560, reg-term: 0.000219}	valid={stereo_loss mean: 2.634716, stereo_loss median: 2.634716}
 [mixture]	Epoch 234 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.3h)	train={loss: 2.606210, max-gradnorm: nan, reg-term: 0.000219}	valid={stereo_loss mean: 2.636954, stereo_loss median: 2.636954}
 [mixture]	Epoch 235 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.3h)	train={loss: 2.603292, max-gradnorm: 8.686962, reg-term: 0.000219}	valid={stereo_loss mean: 2.634007, stereo_loss median: 2.634007}
 [mixture]	Epoch 236 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.3h)	train={loss: 2.644146, max-gradnorm: 12.843273, reg-term: 0.000219}	valid={stereo_loss mean: 2.638044, stereo_loss median: 2.638044}
 [mixture]	Epoch 237 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.2h)	train={loss: 2.578276, max-gradnorm: 43.726444, reg-term: 0.000219}	valid={stereo_loss mean: 2.637356, stereo_loss median: 2.637356}
 [mixture]	Epoch 238 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.2h)	train={loss: 2.592049, max-gradnorm: 14.435508, reg-term: 0.000219}	valid={stereo_loss mean: 2.638756, stereo_loss median: 2.638756}
 [mixture]	Epoch 239 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.2h)	train={loss: 2.630089, max-gradnorm: 19.290514, reg-term: 0.000219}	valid={stereo_loss mean: 2.639212, stereo_loss median: 2.639212}
 [mixture]	Epoch 240 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.2h)	train={loss: 2.607676, max-gradnorm: nan, reg-term: 0.000219}	valid={stereo_loss mean: 2.636741, stereo_loss median: 2.636741}
 [mixture]	Epoch 241 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.1h)	train={loss: 2.567145, max-gradnorm: 18.381895, reg-term: 0.000219}	valid={stereo_loss mean: 2.636196, stereo_loss median: 2.636196}
 [mixture]	Epoch 242 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.1h)	train={loss: 2.536387, max-gradnorm: 11.452466, reg-term: 0.000219}	valid={stereo_loss mean: 2.635316, stereo_loss median: 2.635316}
 [mixture]	Epoch 243 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.1h)	train={loss: 2.521958, max-gradnorm: 21.780279, reg-term: 0.000219}	valid={stereo_loss mean: 2.638617, stereo_loss median: 2.638617}
 [mixture]	Epoch 244 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.0h)	train={loss: 2.588957, max-gradnorm: 24.061895, reg-term: 0.000219}	valid={stereo_loss mean: 2.641481, stereo_loss median: 2.641481}
 [mixture]	Epoch 245 of 311 took 1.75m + 0.07m + 0.01m (finished in 2.0h)	train={loss: 2.671052, max-gradnorm: 14.081613, reg-term: 0.000219}	valid={stereo_loss mean: 2.638052, stereo_loss median: 2.638052}
 [mixture]	Epoch 246 of 311 took 1.75m + 0.06m + 0.01m (finished in 2.0h)	train={loss: 2.551653, max-gradnorm: 51.486755, reg-term: 0.000219}	valid={stereo_loss mean: 2.637999, stereo_loss median: 2.637999}
 [mixture]	Epoch 247 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.9h)	train={loss: 2.575460, max-gradnorm: 17.617935, reg-term: 0.000219}	valid={stereo_loss mean: 2.642150, stereo_loss median: 2.642150}
 [mixture]	Epoch 248 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.9h)	train={loss: 2.572578, max-gradnorm: 46.240295, reg-term: 0.000219}	valid={stereo_loss mean: 2.639651, stereo_loss median: 2.639651}
 [mixture]	Epoch 249 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.9h)	train={loss: 2.611943, max-gradnorm: nan, reg-term: 0.000219}	valid={stereo_loss mean: 2.639791, stereo_loss median: 2.639791}
 [mixture]	Epoch 250 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.8h)	train={loss: 2.594355, max-gradnorm: 28.703100, reg-term: 0.000219}	valid={stereo_loss mean: 2.641400, stereo_loss median: 2.641400}
 [mixture]	Epoch 251 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.8h)	train={loss: 2.602571, max-gradnorm: 27.920830, reg-term: 0.000219}	valid={stereo_loss mean: 2.636591, stereo_loss median: 2.636591}
 [mixture]	Epoch 252 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.8h)	train={loss: 2.588725, max-gradnorm: 24.205135, reg-term: 0.000219}	valid={stereo_loss mean: 2.636441, stereo_loss median: 2.636441}
 [mixture]	Epoch 253 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.8h)	train={loss: 2.644367, max-gradnorm: 86.598602, reg-term: 0.000219}	valid={stereo_loss mean: 2.636248, stereo_loss median: 2.636248}
 [mixture]	Epoch 254 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.7h)	train={loss: 2.537059, max-gradnorm: 102.476318, reg-term: 0.000219}	valid={stereo_loss mean: 2.636837, stereo_loss median: 2.636837}
 [mixture]	Epoch 255 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.7h)	train={loss: 2.591842, max-gradnorm: 63.810875, reg-term: 0.000219}	valid={stereo_loss mean: 2.638492, stereo_loss median: 2.638492}
 [mixture]	Epoch 256 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.7h)	train={loss: 2.484927, max-gradnorm: 51.596153, reg-term: 0.000219}	valid={stereo_loss mean: 2.635203, stereo_loss median: 2.635203}
 [mixture]	Epoch 257 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.6h)	train={loss: 2.593449, max-gradnorm: 54.820293, reg-term: 0.000219}	valid={stereo_loss mean: 2.636575, stereo_loss median: 2.636575}
 [mixture]	Epoch 258 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.6h)	train={loss: 2.625453, max-gradnorm: 15.423173, reg-term: 0.000219}	valid={stereo_loss mean: 2.636863, stereo_loss median: 2.636863}
 [mixture]	Epoch 259 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.6h)	train={loss: 2.528421, max-gradnorm: 22.145695, reg-term: 0.000219}	valid={stereo_loss mean: 2.637394, stereo_loss median: 2.637394}
 [mixture]	Epoch 260 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.5h)	train={loss: 2.606772, max-gradnorm: 18.755276, reg-term: 0.000219}	valid={stereo_loss mean: 2.636995, stereo_loss median: 2.636995}
 [mixture]	Epoch 261 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.5h)	train={loss: 2.533084, max-gradnorm: 17.063887, reg-term: 0.000219}	valid={stereo_loss mean: 2.636516, stereo_loss median: 2.636516}
 [mixture]	Epoch 262 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.5h)	train={loss: 2.617139, max-gradnorm: 21.561344, reg-term: 0.000219}	valid={stereo_loss mean: 2.636040, stereo_loss median: 2.636040}
 [mixture]	Epoch 263 of 311 took 1.74m + 0.07m + 0.01m (finished in 1.5h)	train={loss: 2.604768, max-gradnorm: 43.683842, reg-term: 0.000219}	valid={stereo_loss mean: 2.636552, stereo_loss median: 2.636552}
 [mixture]	Epoch 264 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.4h)	train={loss: 2.614542, max-gradnorm: 19.415039, reg-term: 0.000219}	valid={stereo_loss mean: 2.637079, stereo_loss median: 2.637079}
 [mixture]	Epoch 265 of 311 took 1.74m + 0.07m + 0.01m (finished in 1.4h)	train={loss: 2.596788, max-gradnorm: 17.541952, reg-term: 0.000219}	valid={stereo_loss mean: 2.636170, stereo_loss median: 2.636170}
 [mixture]	Epoch 266 of 311 took 1.74m + 0.07m + 0.01m (finished in 1.4h)	train={loss: 2.680804, max-gradnorm: 76.611488, reg-term: 0.000219}	valid={stereo_loss mean: 2.638409, stereo_loss median: 2.638409}
 [mixture]	Epoch 267 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.3h)	train={loss: 2.525473, max-gradnorm: 47.456829, reg-term: 0.000219}	valid={stereo_loss mean: 2.636211, stereo_loss median: 2.636211}
 [mixture]	Epoch 268 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.3h)	train={loss: 2.644753, max-gradnorm: 20.293322, reg-term: 0.000219}	valid={stereo_loss mean: 2.637108, stereo_loss median: 2.637108}
 [mixture]	Epoch 269 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.3h)	train={loss: 2.609573, max-gradnorm: 22.147892, reg-term: 0.000219}	valid={stereo_loss mean: 2.635912, stereo_loss median: 2.635912}
 [mixture]	Epoch 270 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.2h)	train={loss: 2.605922, max-gradnorm: 13.674175, reg-term: 0.000219}	valid={stereo_loss mean: 2.635180, stereo_loss median: 2.635180}
 [mixture]	Epoch 271 of 311 took 1.75m + 0.06m + 0.01m (finished in 1.2h)	train={loss: 2.573479, max-gradnorm: 13.161443, reg-term: 0.000219}	valid={stereo_loss mean: 2.638716, stereo_loss median: 2.638716}
 [mixture]	Epoch 272 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.2h)	train={loss: 2.550773, max-gradnorm: nan, reg-term: 0.000219}	valid={stereo_loss mean: 2.636297, stereo_loss median: 2.636297}
 [mixture]	Epoch 273 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.1h)	train={loss: 2.518966, max-gradnorm: 16.762941, reg-term: 0.000219}	valid={stereo_loss mean: 2.636365, stereo_loss median: 2.636365}
 [mixture]	Epoch 274 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.1h)	train={loss: 2.663949, max-gradnorm: 18.447674, reg-term: 0.000219}	valid={stereo_loss mean: 2.636031, stereo_loss median: 2.636031}
 [mixture]	Epoch 275 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.1h)	train={loss: 2.504730, max-gradnorm: 41.064114, reg-term: 0.000219}	valid={stereo_loss mean: 2.635873, stereo_loss median: 2.635873}
 [mixture]	Epoch 276 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.1h)	train={loss: 2.626242, max-gradnorm: 23.387501, reg-term: 0.000219}	valid={stereo_loss mean: 2.635563, stereo_loss median: 2.635563}
 [mixture]	Epoch 277 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.0h)	train={loss: 2.589227, max-gradnorm: 37.186947, reg-term: 0.000219}	valid={stereo_loss mean: 2.634466, stereo_loss median: 2.634466}
 [mixture]	Epoch 278 of 311 took 1.75m + 0.07m + 0.01m (finished in 1.0h)	train={loss: 2.590516, max-gradnorm: 22.425655, reg-term: 0.000219}	valid={stereo_loss mean: 2.634432, stereo_loss median: 2.634432}
 [mixture]	Epoch 279 of 311 took 1.74m + 0.06m + 0.01m (finished in 1.0h)	train={loss: 2.633280, max-gradnorm: 104.954910, reg-term: 0.000219}	valid={stereo_loss mean: 2.635577, stereo_loss median: 2.635577}
 [mixture]	Epoch 280 of 311 took 1.74m + 0.07m + 0.01m (finished in 0.9h)	train={loss: 2.648216, max-gradnorm: 36.684494, reg-term: 0.000219}	valid={stereo_loss mean: 2.635780, stereo_loss median: 2.635780}
 [mixture]	Epoch 281 of 311 took 1.74m + 0.06m + 0.01m (finished in 0.9h)	train={loss: 2.567731, max-gradnorm: 33.893600, reg-term: 0.000219}	valid={stereo_loss mean: 2.635531, stereo_loss median: 2.635531}
 [mixture]	Epoch 282 of 311 took 1.74m + 0.07m + 0.01m (finished in 0.9h)	train={loss: 2.518914, max-gradnorm: 23.023661, reg-term: 0.000219}	valid={stereo_loss mean: 2.635027, stereo_loss median: 2.635027}
 [mixture]	Epoch 283 of 311 took 1.75m + 0.06m + 0.01m (finished in 0.8h)	train={loss: 2.586980, max-gradnorm: 79.792381, reg-term: 0.000219}	valid={stereo_loss mean: 2.634823, stereo_loss median: 2.634823}
 [mixture]	Epoch 284 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.8h)	train={loss: 2.635366, max-gradnorm: 15.936694, reg-term: 0.000219}	valid={stereo_loss mean: 2.635785, stereo_loss median: 2.635785}
 [mixture]	Epoch 285 of 311 took 1.74m + 0.07m + 0.01m (finished in 0.8h)	train={loss: 2.581056, max-gradnorm: 72.973373, reg-term: 0.000219}	valid={stereo_loss mean: 2.635539, stereo_loss median: 2.635539}
 [mixture]	Epoch 286 of 311 took 1.74m + 0.07m + 0.01m (finished in 0.8h)	train={loss: 2.627419, max-gradnorm: 28.086542, reg-term: 0.000219}	valid={stereo_loss mean: 2.635556, stereo_loss median: 2.635556}
 [mixture]	Epoch 287 of 311 took 1.74m + 0.07m + 0.01m (finished in 0.7h)	train={loss: 2.695916, max-gradnorm: 17.417349, reg-term: 0.000219}	valid={stereo_loss mean: 2.634743, stereo_loss median: 2.634743}
 [mixture]	Epoch 288 of 311 took 1.74m + 0.06m + 0.01m (finished in 0.7h)	train={loss: 2.530678, max-gradnorm: 18.011274, reg-term: 0.000219}	valid={stereo_loss mean: 2.634762, stereo_loss median: 2.634762}
 [mixture]	Epoch 289 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.7h)	train={loss: 2.516539, max-gradnorm: 25.969553, reg-term: 0.000219}	valid={stereo_loss mean: 2.635616, stereo_loss median: 2.635616}
 [mixture]	Epoch 290 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.6h)	train={loss: 2.610115, max-gradnorm: 31.172583, reg-term: 0.000219}	valid={stereo_loss mean: 2.635001, stereo_loss median: 2.635001}
 [mixture]	Epoch 291 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.6h)	train={loss: 2.558674, max-gradnorm: 15.791024, reg-term: 0.000219}	valid={stereo_loss mean: 2.636353, stereo_loss median: 2.636353}
 [mixture]	Epoch 292 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.6h)	train={loss: 2.605485, max-gradnorm: 78.147034, reg-term: 0.000219}	valid={stereo_loss mean: 2.635480, stereo_loss median: 2.635480}
 [mixture]	Epoch 293 of 311 took 1.75m + 0.06m + 0.01m (finished in 0.5h)	train={loss: 2.600717, max-gradnorm: 12.016582, reg-term: 0.000219}	valid={stereo_loss mean: 2.634878, stereo_loss median: 2.634878}
 [mixture]	Epoch 294 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.5h)	train={loss: 2.589171, max-gradnorm: 12.397815, reg-term: 0.000219}	valid={stereo_loss mean: 2.634926, stereo_loss median: 2.634926}
 [mixture]	Epoch 295 of 311 took 1.76m + 0.06m + 0.01m (finished in 0.5h)	train={loss: 2.650586, max-gradnorm: 34.172993, reg-term: 0.000219}	valid={stereo_loss mean: 2.635846, stereo_loss median: 2.635846}
 [mixture]	Epoch 296 of 311 took 1.76m + 0.07m + 0.01m (finished in 0.5h)	train={loss: 2.579758, max-gradnorm: 28.872366, reg-term: 0.000219}	valid={stereo_loss mean: 2.634395, stereo_loss median: 2.634395}
 [mixture]	Epoch 297 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.4h)	train={loss: 2.723576, max-gradnorm: 34.275017, reg-term: 0.000219}	valid={stereo_loss mean: 2.635022, stereo_loss median: 2.635022}
 [mixture]	Epoch 298 of 311 took 1.75m + 0.06m + 0.01m (finished in 0.4h)	train={loss: 2.621506, max-gradnorm: 336.446960, reg-term: 0.000219}	valid={stereo_loss mean: 2.634667, stereo_loss median: 2.634667}
 [mixture]	Epoch 299 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.4h)	train={loss: 2.596022, max-gradnorm: 10.515905, reg-term: 0.000219}	valid={stereo_loss mean: 2.636234, stereo_loss median: 2.636234}
 [mixture]	Epoch 300 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.3h)	train={loss: 2.559980, max-gradnorm: 16.390991, reg-term: 0.000219}	valid={stereo_loss mean: 2.634866, stereo_loss median: 2.634866}
 [mixture]	Epoch 301 of 311 took 1.76m + 0.07m + 0.01m (finished in 0.3h)	train={loss: 2.546614, max-gradnorm: 16.110189, reg-term: 0.000219}	valid={stereo_loss mean: 2.635095, stereo_loss median: 2.635095}
 [mixture]	Epoch 302 of 311 took 1.75m + 0.06m + 0.01m (finished in 0.3h)	train={loss: 2.578168, max-gradnorm: 34.128605, reg-term: 0.000219}	valid={stereo_loss mean: 2.634595, stereo_loss median: 2.634595}
 [mixture]	Epoch 303 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.2h)	train={loss: 2.569503, max-gradnorm: 29.592957, reg-term: 0.000219}	valid={stereo_loss mean: 2.634712, stereo_loss median: 2.634712}
 [mixture]	Epoch 304 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.2h)	train={loss: 2.614694, max-gradnorm: 23.939196, reg-term: 0.000219}	valid={stereo_loss mean: 2.634388, stereo_loss median: 2.634388}
 [mixture]	Epoch 305 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.2h)	train={loss: 2.485431, max-gradnorm: 13.183243, reg-term: 0.000219}	valid={stereo_loss mean: 2.634456, stereo_loss median: 2.634456}
 [mixture]	Epoch 306 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.2h)	train={loss: 2.613657, max-gradnorm: 39.520161, reg-term: 0.000219}	valid={stereo_loss mean: 2.634631, stereo_loss median: 2.634631}
 [mixture]	Epoch 307 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.1h)	train={loss: 2.596820, max-gradnorm: 17.336699, reg-term: 0.000219}	valid={stereo_loss mean: 2.634718, stereo_loss median: 2.634718}
 [mixture]	Epoch 308 of 311 took 1.75m + 0.06m + 0.01m (finished in 0.1h)	train={loss: 2.557238, max-gradnorm: 153.557312, reg-term: 0.000219}	valid={stereo_loss mean: 2.634601, stereo_loss median: 2.634601}
 [mixture]	Epoch 309 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.1h)	train={loss: 2.554148, max-gradnorm: 16.193178, reg-term: 0.000219}	valid={stereo_loss mean: 2.634714, stereo_loss median: 2.634714}
 [mixture]	Epoch 310 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.0h)	train={loss: 2.539046, max-gradnorm: 18.486721, reg-term: 0.000219}	valid={stereo_loss mean: 2.634593, stereo_loss median: 2.634593}
 [mixture]	Epoch 311 of 311 took 1.75m + 0.07m + 0.01m (finished in 0.0h)	train={loss: 2.641060, max-gradnorm: 11.440268, reg-term: 0.000219}	valid={stereo_loss mean: 2.634452, stereo_loss median: 2.634452}
